{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Multiclass margin classifier {#multiclass_margin_classifier}\n============================\n\n::: {.meta}\n:property=\\\"og:description\\\": Using PyTorch to implement a multiclass\nquantum variational classifier on MNIST data. :property=\\\"og:image\\\":\n<https://pennylane.ai/qml/_images/margin_2.png>\n:::\n\n::: {.related}\ntutorial\\_variational\\_classifier Variational classifier\ntutorial\\_data\\_reuploading\\_classifier Data-reuploading classifier\n:::\n\n*Author: Safwan Hossein --- Posted: 09 April 2020. Last updated: 28\nJanuary 2021.*\n\nIn this tutorial, we show how to use the PyTorch interface for PennyLane\nto implement a multiclass variational classifier. We consider the iris\ndatabase from UCI, which has 4 features and 3 classes. We use multiple\none-vs-all classifiers with a margin loss (see [Multiclass Linear\nSVM](http://cs231n.github.io/linear-classify/)) to classify data. Each\nclassifier is implemented on an individual variational circuit, whose\narchitecture is inspired by [Farhi and Neven\n(2018)](https://arxiv.org/abs/1802.06002) as well as [Schuld et al.\n(2018)](https://arxiv.org/abs/1804.00633).\n\n| \n\n![](../demonstrations/multiclass_classification/margin_2.png){.align-center\nwidth=\"50.0%\"}\n\n| \n\nInitial Setup\n-------------\n\nWe import PennyLane, the PennyLane-provided version of NumPy, relevant\ntorch modules, and define the constants that will be used in this\ntutorial.\n\nOur feature size is 4, and we will use amplitude embedding. This means\nthat each possible amplitude (in the computational basis) will\ncorrespond to a single feature. With 2 qubits (wires), there are 4\npossible states, and as such, we can encode a feature vector of size 4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\nimport torch\nimport numpy as np\nfrom torch.autograd import Variable\nimport torch.optim as optim\n\nnp.random.seed(0)\ntorch.manual_seed(0)\n\nnum_classes = 3\nmargin = 0.15\nfeature_size = 4\nbatch_size = 10\nlr_adam = 0.01\ntrain_split = 0.75\n# the number of the required qubits is calculated from the number of features\nnum_qubits = int(np.ceil(np.log2(feature_size)))\nnum_layers = 6\ntotal_iterations = 100\n\ndev = qml.device(\"default.qubit\", wires=num_qubits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantum Circuit\n===============\n\nWe first create the layer that will be repeated in our variational\nquantum circuits. It consists of rotation gates for each qubit, followed\nby entangling/CNOT gates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def layer(W):\n    for i in range(num_qubits):\n        qml.Rot(W[i, 0], W[i, 1], W[i, 2], wires=i)\n    for j in range(num_qubits - 1):\n        qml.CNOT(wires=[j, j + 1])\n    if num_qubits >= 2:\n        # Apply additional CNOT to entangle the last with the first qubit\n        qml.CNOT(wires=[num_qubits - 1, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now define the quantum nodes that will be used. As we are\nimplementing our multiclass classifier as multiple one-vs-all\nclassifiers, we will use 3 QNodes, each representing one such\nclassifier. That is, `circuit1` classifies if a sample belongs to class\n1 or not, and so on. The circuit architecture for all nodes are the\nsame. We use the PyTorch interface for the QNodes. Data is embedded in\neach circuit using amplitude embedding.\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nFor demonstration purposes we are using a very simple circuit here. You\nmay find that other choices, for example more elaborate measurements,\nincrease the power of the classifier.\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def circuit(weights, feat=None):\n    qml.AmplitudeEmbedding(feat, range(num_qubits), pad_with=0.0, normalize=True)\n\n    for W in weights:\n        layer(W)\n\n    return qml.expval(qml.PauliZ(0))\n\n\nqnodes = []\nfor iq in range(num_classes):\n    qnode = qml.QNode(circuit, dev, interface=\"torch\")\n    qnodes.append(qnode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The variational quantum circuit is parametrized by the weights. We use a\nclassical bias term that is applied after processing the quantum\ncircuit\\'s output. Both variational circuit weights and classical bias\nterm are optimized.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def variational_classifier(q_circuit, params, feat):\n    weights = params[0]\n    bias = params[1]\n    return q_circuit(weights, feat=feat) + bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loss Function\n=============\n\nImplementing multiclass classifiers as a number of one-vs-all\nclassifiers generally evokes using the margin loss. The output of the\n$i$ th classifier, $c_i$ on input $x$ is interpreted as a score, $s_i$\nbetween \\[-1,1\\]. More concretely, we have:\n\n$$s_i = c_i(x; \\theta)$$\n\nThe multiclass margin loss attempts to ensure that the score for the\ncorrect class is higher than that of incorrect classes by some margin.\nFor a sample $(x,y)$ where $y$ denotes the class label, we can\nanalytically express the mutliclass loss on this sample as:\n\n$$L(x,y) = \\sum_{j \\ne y}{\\max{\\left(0, s_j - s_y + \\Delta)\\right)}}$$\n\nwhere $\\Delta$ denotes the margin. The margin parameter is chosen as a\nhyperparameter. For more information, see [Multiclass Linear\nSVM](http://cs231n.github.io/linear-classify/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def multiclass_svm_loss(q_circuits, all_params, feature_vecs, true_labels):\n    loss = 0\n    num_samples = len(true_labels)\n    for i, feature_vec in enumerate(feature_vecs):\n        # Compute the score given to this sample by the classifier corresponding to the\n        # true label. So for a true label of 1, get the score computed by classifer 1,\n        # which distinguishes between \"class 1\" or \"not class 1\".\n        s_true = variational_classifier(\n            q_circuits[int(true_labels[i])],\n            (all_params[0][int(true_labels[i])], all_params[1][int(true_labels[i])]),\n            feature_vec,\n        )\n        s_true = s_true.float()\n        li = 0\n\n        # Get the scores computed for this sample by the other classifiers\n        for j in range(num_classes):\n            if j != int(true_labels[i]):\n                s_j = variational_classifier(\n                    q_circuits[j], (all_params[0][j], all_params[1][j]), feature_vec\n                )\n                s_j = s_j.float()\n                li += torch.max(torch.zeros(1).float(), s_j - s_true + margin)\n        loss += li\n\n    return loss / num_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classification Function\n=======================\n\nNext, we use the learned models to classify our samples. For a given\nsample, compute the score given to it by classifier $i$, which\nquantifies how likely it is that this sample belongs to class $i$. For\neach sample, return the class with the highest score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def classify(q_circuits, all_params, feature_vecs, labels):\n    predicted_labels = []\n    for i, feature_vec in enumerate(feature_vecs):\n        scores = np.zeros(num_classes)\n        for c in range(num_classes):\n            score = variational_classifier(\n                q_circuits[c], (all_params[0][c], all_params[1][c]), feature_vec\n            )\n            scores[c] = float(score)\n        pred_class = np.argmax(scores)\n        predicted_labels.append(pred_class)\n    return predicted_labels\n\n\ndef accuracy(labels, hard_predictions):\n    loss = 0\n    for l, p in zip(labels, hard_predictions):\n        if torch.abs(l - p) < 1e-5:\n            loss = loss + 1\n    loss = loss / labels.shape[0]\n    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Loading and Processing\n===========================\n\nNow we load in the iris dataset and normalize the features so that the\nsum of the feature elements squared is 1 ($\\ell_2$ norm is 1).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def load_and_process_data():\n    data = np.loadtxt(\"multiclass_classification/iris.csv\", delimiter=\",\")\n    X = torch.tensor(data[:, 0:feature_size])\n    print(\"First X sample, original  :\", X[0])\n\n    # normalize each input\n    normalization = torch.sqrt(torch.sum(X ** 2, dim=1))\n    X_norm = X / normalization.reshape(len(X), 1)\n    print(\"First X sample, normalized:\", X_norm[0])\n\n    Y = torch.tensor(data[:, -1])\n    return X, Y\n\n\n# Create a train and test split.\ndef split_data(feature_vecs, Y):\n    num_data = len(Y)\n    num_train = int(train_split * num_data)\n    index = np.random.permutation(range(num_data))\n    feat_vecs_train = feature_vecs[index[:num_train]]\n    Y_train = Y[index[:num_train]]\n    feat_vecs_test = feature_vecs[index[num_train:]]\n    Y_test = Y[index[num_train:]]\n    return feat_vecs_train, feat_vecs_test, Y_train, Y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Procedure\n==================\n\nIn the training procedure, we begin by first initializing randomly the\nparameters we wish to learn (variational circuit weights and classical\nbias). As these are the variables we wish to optimize, we set the\n`requires_grad` flag to `True`. We use minibatch training---the average\nloss for a batch of samples is computed, and the optimization step is\nbased on this. Total training time with the default parameters is\nroughly 15 minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def training(features, Y):\n    num_data = Y.shape[0]\n    feat_vecs_train, feat_vecs_test, Y_train, Y_test = split_data(features, Y)\n    num_train = Y_train.shape[0]\n    q_circuits = qnodes\n\n    # Initialize the parameters\n    all_weights = [\n        Variable(0.1 * torch.randn(num_layers, num_qubits, 3), requires_grad=True)\n        for i in range(num_classes)\n    ]\n    all_bias = [Variable(0.1 * torch.ones(1), requires_grad=True) for i in range(num_classes)]\n    optimizer = optim.Adam(all_weights + all_bias, lr=lr_adam)\n    params = (all_weights, all_bias)\n    print(\"Num params: \", 3 * num_layers * num_qubits * 3 + 3)\n\n    costs, train_acc, test_acc = [], [], []\n\n    # train the variational classifier\n    for it in range(total_iterations):\n        batch_index = np.random.randint(0, num_train, (batch_size,))\n        feat_vecs_train_batch = feat_vecs_train[batch_index]\n        Y_train_batch = Y_train[batch_index]\n\n        optimizer.zero_grad()\n        curr_cost = multiclass_svm_loss(q_circuits, params, feat_vecs_train_batch, Y_train_batch)\n        curr_cost.backward()\n        optimizer.step()\n\n        # Compute predictions on train and validation set\n        predictions_train = classify(q_circuits, params, feat_vecs_train, Y_train)\n        predictions_test = classify(q_circuits, params, feat_vecs_test, Y_test)\n        acc_train = accuracy(Y_train, predictions_train)\n        acc_test = accuracy(Y_test, predictions_test)\n\n        print(\n            \"Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc test: {:0.7f} \"\n            \"\".format(it + 1, curr_cost.item(), acc_train, acc_test)\n        )\n\n        costs.append(curr_cost.item())\n        train_acc.append(acc_train)\n        test_acc.append(acc_test)\n\n    return costs, train_acc, test_acc\n\n\n# We now run our training algorithm and plot the results. Note that\n# for plotting, the matplotlib library is required\n\nfeatures, Y = load_and_process_data()\ncosts, train_acc, test_acc = training(features, Y)\n\nimport matplotlib.pyplot as plt\n\nfig, ax1 = plt.subplots()\niters = np.arange(0, total_iterations, 1)\ncolors = [\"tab:red\", \"tab:blue\"]\nax1.set_xlabel(\"Iteration\", fontsize=17)\nax1.set_ylabel(\"Cost\", fontsize=17, color=colors[0])\nax1.plot(iters, costs, color=colors[0], linewidth=4)\nax1.tick_params(axis=\"y\", labelsize=14, labelcolor=colors[0])\n\nax2 = ax1.twinx()\nax2.set_ylabel(\"Test Acc.\", fontsize=17, color=colors[1])\nax2.plot(iters, test_acc, color=colors[1], linewidth=4)\n\nax2.tick_params(axis=\"x\", labelsize=14)\nax2.tick_params(axis=\"y\", labelsize=14, labelcolor=colors[1])\n\nplt.grid(False)\nplt.tight_layout()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}