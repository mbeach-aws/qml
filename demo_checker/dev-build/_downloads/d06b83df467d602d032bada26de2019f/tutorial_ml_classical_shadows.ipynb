{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Machine learning for quantum many-body problems\n===============================================\n\n::: {.meta}\n:property=\\\"og:description\\\": Machine learning for many-body problems\n:property=\\\"og:image\\\":\n<https://pennylane.ai/qml/_images/ml_classical_shadow.png>\n:::\n\n::: {.related}\ntutorial\\_classical\\_shadows Classical Shadows\ntutorial\\_kernel\\_based\\_training Kernel-based training with\nscikit-learn tutorial\\_kernels\\_module Training and evaluating quantum\nkernels\n:::\n\n*Author: Utkarsh Azad --- Posted: 02 May 2022. Last Updated: 09 May\n2022*\n\nStoring and processing a complete description of an $n$-qubit quantum\nmechanical system is challenging because the amount of memory required\ngenerally scales exponentially with the number of qubits. The quantum\ncommunity has recently addressed this challenge by using the\n`classical shadow <tutorial_classical_shadows>`{.interpreted-text\nrole=\"doc\"} formalism, which allows us to build more concise classical\ndescriptions of quantum states using randomized single-qubit\nmeasurements. It was argued in Ref. that combining classical shadows\nwith classical machine learning enables using learning models that\nefficiently predict properties of the quantum systems, such as the\nexpectation value of a Hamiltonian, correlation functions, and\nentanglement entropies.\n\n![Combining machine learning and classical\nshadows](/demonstrations/ml_classical_shadows/class_shadow_ml.png){.align-center\nwidth=\"80.0%\"}\n\nIn this demo, we describe one of the ideas presented in Ref. for using\nclassical shadow formalism and machine learning to predict the\nground-state properties of the 2D antiferromagnetic Heisenberg model. We\nbegin by learning how to build the Heisenberg model, calculate its\nground-state properties, and compute its classical shadow. Finally, we\ndemonstrate how to use\n`kernel-based learning models <tutorial_kernels_module>`{.interpreted-text\nrole=\"doc\"} to predict ground-state properties from the learned\nclassical shadows. So let\\'s get started!\n\nBuilding the 2D Heisenberg Model\n--------------------------------\n\nWe define a two-dimensional antiferromagnetic [Heisenberg\nmodel](https://en.wikipedia.org/wiki/Quantum_Heisenberg_model) as a\nsquare lattice, where a spin-1/2 particle occupies each site. The\nantiferromagnetic nature and the overall physics of this model depend on\nthe couplings $J_{ij}$ present between the spins, as reflected in the\nHamiltonian associated with the model:\n\n$$H = \\sum_{i < j} J_{ij}(X_i X_j + Y_i Y_j + Z_i Z_j) .$$\n\nHere, we consider the family of Hamiltonians where all the couplings\n$J_{ij}$ are sampled uniformly from \\[0, 2\\]. We build a coupling matrix\n$J$ by providing the number of rows $N_r$ and columns $N_c$ present in\nthe square lattice. The dimensions of this matrix are $N_s \\times N_s$,\nwhere $N_s = N_r \\times N_c$ is the total number of spin particles\npresent in the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import itertools as it\nimport pennylane.numpy as np\nimport numpy as anp\n\ndef build_coupling_mats(num_mats, num_rows, num_cols):\n    num_spins = num_rows * num_cols\n    coupling_mats = np.zeros((num_mats, num_spins, num_spins))\n    coup_terms = anp.random.RandomState(24).uniform(0, 2,\n                        size=(num_mats, 2 * num_rows * num_cols - num_rows - num_cols))\n    # populate edges to build the grid lattice\n    edges = [(si, sj) for (si, sj) in it.combinations(range(num_spins), 2)\n                        if sj % num_cols and sj - si == 1 or sj - si == num_cols]\n    for itr in range(num_mats):\n        for ((i, j), term) in zip(edges, coup_terms[itr]):\n            coupling_mats[itr][i][j] = coupling_mats[itr][j][i] = term\n    return coupling_mats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this demo, we study a model with four spins arranged on the nodes of\na square lattice. We require four qubits for simulating this model; one\nqubit for each spin. We start by building a coupling matrix `J_mat`\nusing our previously defined function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Nr, Nc = 2, 2\nnum_qubits = Nr * Nc  # Ns\nJ_mat = build_coupling_mats(1, Nr, Nc)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now visualize the model instance by representing the coupling\nmatrix as a `networkx` graph:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport networkx as nx\n\nG = nx.from_numpy_matrix(np.matrix(J_mat), create_using=nx.DiGraph)\npos = {i: (i % Nc, -(i // Nc)) for i in G.nodes()}\nedge_labels = {(x, y): np.round(J_mat[x, y], 2) for x, y in G.edges()}\nweights = [x + 1.5 for x in list(nx.get_edge_attributes(G, \"weight\").values())]\n\nplt.figure(figsize=(4, 4))\nnx.draw(\n    G, pos, node_color=\"lightblue\", with_labels=True,\n    node_size=600, width=weights, edge_color=\"firebrick\",\n)\nnx.draw_networkx_edge_labels(G, pos=pos, edge_labels=edge_labels)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then use the same coupling matrix `J_mat` to obtain the Hamiltonian\n$H$ for the model we have instantiated above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\n\ndef Hamiltonian(J_mat):\n    coeffs, ops = [], []\n    ns = J_mat.shape[0]\n    for i, j in it.combinations(range(ns), r=2):\n        coeff = J_mat[i, j]\n        if coeff:\n            for op in [qml.PauliX, qml.PauliY, qml.PauliZ]:\n                coeffs.append(coeff)\n                ops.append(op(i) @ op(j))\n    H = qml.Hamiltonian(coeffs, ops)\n    return H\n\nprint(f\"Hamiltonian =\\n{Hamiltonian(J_mat)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the Heisenberg model, a property of interest is usually the two-body\ncorrelation function $C_{ij}$, which for a pair of spins $i$ and $j$ is\ndefined as the following operator:\n\n$$\\hat{C}_{ij} = \\frac{1}{3} (X_i X_j + Y_iY_j + Z_iZ_j).$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def corr_function(i, j):\n    ops = []\n    for op in [qml.PauliX, qml.PauliY, qml.PauliZ]:\n        if i != j:\n            ops.append(op(i) @ op(j))\n        else:\n            ops.append(qml.Identity(i))\n    return ops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The expectation value of each such operator $\\hat{C}_{ij}$ with respect\nto the ground state $|\\psi_{0}\\rangle$ of the model can be used to build\nthe correlation matrix $C$:\n\n$${C}_{ij} = \\langle \\hat{C}_{ij} \\rangle = \\frac{1}{3} \\langle \\psi_{0} | X_i X_j + Y_iY_j + Z_iZ_j | \\psi_{0} \\rangle .$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hence, to build $C$ for the model, we need to calculate its ground state\n$|\\psi_{0}\\rangle$. We do this by diagonalizing the Hamiltonian for the\nmodel. Then, we obtain the eigenvector corresponding to the smallest\neigenvalue.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import scipy as sp\n\nham = Hamiltonian(J_mat)\neigvals, eigvecs = sp.sparse.linalg.eigs(ham.sparse_matrix())\npsi0 = eigvecs[:, np.argmin(eigvals)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then build a circuit that initializes the qubits into the ground\nstate and measures the expectation value of the provided set of\nobservables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev_exact = qml.device(\"default.qubit\", wires=num_qubits) # for exact simulation\n\ndef circuit(psi, observables):\n    psi = psi / np.linalg.norm(psi) # normalize the state\n    qml.QubitStateVector(psi, wires=range(num_qubits))\n    return [qml.expval(o) for o in observables]\n\ncircuit_exact = qml.QNode(circuit, dev_exact)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we execute this circuit to obtain the exact correlation matrix\n$C$. We compute the correlation operators $\\hat{C}_{ij}$ and their\nexpectation values with respect to the ground state $|\\psi_0\\rangle$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "coups = list(it.product(range(num_qubits), repeat=2))\ncorrs = [corr_function(i, j) for i, j in coups]\n\ndef build_exact_corrmat(coups, corrs, circuit, psi):\n    corr_mat_exact = np.zeros((num_qubits, num_qubits))\n    for idx, (i, j) in enumerate(coups):\n        corr = corrs[idx]\n        if i == j:\n            corr_mat_exact[i][j] = 1.0\n        else:\n            corr_mat_exact[i][j] = (\n                np.sum(np.array([circuit(psi, observables=[o]) for o in corr]).T) / 3\n            )\n            corr_mat_exact[j][i] = corr_mat_exact[i][j]\n    return corr_mat_exact\n\nexpval_exact = build_exact_corrmat(coups, corrs, circuit_exact, psi0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once built, we can visualize the correlation matrix:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\nim = ax.imshow(expval_exact, cmap=plt.get_cmap(\"RdBu\"), vmin=-1, vmax=1)\nax.xaxis.set_ticks(range(num_qubits))\nax.yaxis.set_ticks(range(num_qubits))\nax.xaxis.set_tick_params(labelsize=14)\nax.yaxis.set_tick_params(labelsize=14)\nax.set_title(\"Exact Correlation Matrix\", fontsize=14)\n\nbar = fig.colorbar(im, pad=0.05, shrink=0.80    )\nbar.set_label(r\"$C_{ij}$\", fontsize=14, rotation=0)\nbar.ax.tick_params(labelsize=14)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Constructing Classical Shadows\n==============================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have built the Heisenberg model, the next step is to\nconstruct a\n`classical shadow <tutorial_classical_shadows>`{.interpreted-text\nrole=\"doc\"} representation for its ground state. To construct an\napproximate classical representation of an $n$-qubit quantum state\n$\\rho$, we perform randomized single-qubit measurements on $T$-copies of\n$\\rho$. Each measurement is chosen randomly among the Pauli bases $X$,\n$Y$, or $Z$ to yield random $n$ pure product states $|s_i\\rangle$ for\neach copy:\n\n$$|s_{i}^{(t)}\\rangle \\in \\{|0\\rangle, |1\\rangle, |+\\rangle, |-\\rangle, |i+\\rangle, |i-\\rangle\\}.$$\n\n$$S_T(\\rho) = \\big\\{|s_{i}^{(t)}\\rangle: i\\in\\{1,\\ldots, n\\},\\ t\\in\\{1,\\ldots, T\\} \\big\\}.$$\n\nEach of the $|s_i^{(t)}\\rangle$ provides us with a snapshot of the state\n$\\rho$, and the $nT$ measurements yield the complete set $S_{T}$, which\nrequires just $3nT$ bits to be stored in classical memory. This is\ndiscussed in further detail in our previous demo about\n`classical shadows <tutorial_classical_shadows>`{.interpreted-text\nrole=\"doc\"}.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](/demonstrations/ml_classical_shadows/class_shadow_prep.png){.align-center\nwidth=\"100.0%\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To prepare a classical shadow for the ground state of the Heisenberg\nmodel, we simply reuse the circuit template used above and reconstruct a\n`QNode` utilizing a device that performs single-shot measurements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev_oshot = qml.device(\"default.qubit\", wires=num_qubits, shots=1)\ncircuit_oshot = qml.QNode(circuit, dev_oshot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we define a function to build the classical shadow for the quantum\nstate prepared by a given $n$-qubit circuit using $T$-copies of\nrandomized Pauli basis measurements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def gen_class_shadow(circ_template, circuit_params, num_shadows, num_qubits):\n    # prepare the complete set of available Pauli operators\n    unitary_ops = [qml.PauliX, qml.PauliY, qml.PauliZ]\n    # sample random Pauli measurements uniformly\n    unitary_ensmb = np.random.randint(0, 3, size=(num_shadows, num_qubits), dtype=int)\n\n    outcomes = np.zeros((num_shadows, num_qubits))\n    for ns in range(num_shadows):\n        # for each snapshot, extract the Pauli basis measurement to be performed\n        meas_obs = [unitary_ops[unitary_ensmb[ns, i]](i) for i in range(num_qubits)]\n        # perform single shot randomized Pauli measuremnt for each qubit\n        outcomes[ns, :] = circ_template(circuit_params, observables=meas_obs)\n\n    return outcomes, unitary_ensmb\n\n\noutcomes, basis = gen_class_shadow(circuit_oshot, psi0, 100, num_qubits)\nprint(\"First five measurement outcomes =\\n\", outcomes[:5])\nprint(\"First five measurement bases =\\n\", basis[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Furthermore, $S_{T}$ can be used to construct an approximation of the\nunderlying $n$-qubit state $\\rho$ by averaging over $\\sigma_t$:\n\n$$\\sigma_T(\\rho) = \\frac{1}{T} \\sum_{1}^{T} \\big(3|s_{1}^{(t)}\\rangle\\langle s_1^{(t)}| - \\mathbb{I}\\big)\\otimes \\ldots \\otimes \\big(3|s_{n}^{(t)}\\rangle\\langle s_n^{(t)}| - \\mathbb{I}\\big).$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def snapshot_state(meas_list, obs_list):\n    # undo the rotations done for performing Pauli measurements in the specific basis\n    rotations = [\n        qml.matrix(qml.Hadamard(wires=0)), # X-basis\n        qml.matrix(qml.Hadamard(wires=0)) @ qml.matrix(qml.adjoint(qml.S(wires=0))), # Y-basis\n        qml.matrix(qml.Identity(wires=0)), # Z-basis\n    ]\n\n    # reconstruct snapshot from local Pauli measurements\n    rho_snapshot = [1]\n    for meas_out, basis in zip(meas_list, obs_list):\n        # preparing state |s_i><s_i| using the post measurement outcome:\n        # |0><0| for 1 and |1><1| for -1\n        state = np.array([[1, 0], [0, 0]]) if meas_out == 1 else np.array([[0, 0], [0, 1]])\n        local_rho = 3 * (rotations[basis].conj().T @ state @ rotations[basis]) - np.eye(2)\n        rho_snapshot = np.kron(rho_snapshot, local_rho)\n\n    return rho_snapshot\n\ndef shadow_state_reconst(shadow):\n    num_snapshots, num_qubits = shadow[0].shape\n    meas_lists, obs_lists = shadow\n\n    # Reconstruct the quantum state from its classical shadow\n    shadow_rho = np.zeros((2 ** num_qubits, 2 ** num_qubits), dtype=complex)\n    for i in range(num_snapshots):\n        shadow_rho += snapshot_state(meas_lists[i], obs_lists[i])\n\n    return shadow_rho / num_snapshots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To see how well the reconstruction works for different values of $T$, we\nlook at the\n[fidelity](https://en.wikipedia.org/wiki/Fidelity_of_quantum_states) of\nthe actual quantum state with respect to the reconstructed quantum state\nfrom the classical shadow with $T$ copies. On average, as the number of\ncopies $T$ is increased, the reconstruction becomes more effective with\naverage higher fidelity values (orange) and lower variance (blue).\nEventually, in the limit $T\\rightarrow\\infty$, the reconstruction will\nbe exact.\n\n![Fidelity of the reconstructed ground state with different shadow sizes\n$T$](/demonstrations/ml_classical_shadows/fidel_snapshot.png){.align-center\nwidth=\"80.0%\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The reconstructed quantum state $\\sigma_T$ can also be used to evaluate\nexpectation values $\\text{Tr}(O\\sigma_T)$ for some localized observable\n$O = \\bigotimes_{i}^{n} P_i$, where $P_i \\in \\{I, X, Y, Z\\}$. However,\nas shown above, $\\sigma_T$ would be only an approximation of $\\rho$ for\nfinite values of $T$. Therefore, to estimate $\\langle O \\rangle$\nrobustly, we use the median of means estimation. For this purpose, we\nsplit up the $T$ shadows into $K$ equally-sized groups and evaluate the\nmedian of the mean value of $\\langle O \\rangle$ for each of these\ngroups.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def estimate_shadow_obs(shadow, observable, k=10):\n    shadow_size = shadow[0].shape[0]\n\n    # convert Pennylane observables to indices\n    map_name_to_int = {\"PauliX\": 0, \"PauliY\": 1, \"PauliZ\": 2}\n    if isinstance(observable, (qml.PauliX, qml.PauliY, qml.PauliZ)):\n        target_obs = np.array([map_name_to_int[observable.name]])\n        target_locs = np.array([observable.wires[0]])\n    else:\n        target_obs = np.array([map_name_to_int[o.name] for o in observable.obs])\n        target_locs = np.array([o.wires[0] for o in observable.obs])\n\n    # perform median of means to return the result\n    means = []\n    meas_list, obs_lists = shadow\n    for i in range(0, shadow_size, shadow_size // k):\n        meas_list_k, obs_lists_k = (\n            meas_list[i : i + shadow_size // k],\n            obs_lists[i : i + shadow_size // k],\n        )\n        indices = np.all(obs_lists_k[:, target_locs] == target_obs, axis=1)\n        if sum(indices):\n            means.append(\n                np.sum(np.prod(meas_list_k[indices][:, target_locs], axis=1)) / sum(indices)\n            )\n        else:\n            means.append(0)\n\n    return np.median(means)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we estimate the correlation matrix $C^{\\prime}$ from the classical\nshadow approximation of the ground state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "coups = list(it.product(range(num_qubits), repeat=2))\ncorrs = [corr_function(i, j) for i, j in coups]\nqbobs = [qob for qobs in corrs for qob in qobs]\n\ndef build_estim_corrmat(coups, corrs, num_obs, shadow):\n    k = int(2 * np.log(2 * num_obs)) # group size\n    corr_mat_estim = np.zeros((num_qubits, num_qubits))\n    for idx, (i, j) in enumerate(coups):\n        corr = corrs[idx]\n        if i == j:\n            corr_mat_estim[i][j] = 1.0\n        else:\n            corr_mat_estim[i][j] = (\n                np.sum(np.array([estimate_shadow_obs(shadow, o, k=k+1) for o in corr])) / 3\n            )\n            corr_mat_estim[j][i] = corr_mat_estim[i][j]\n    return corr_mat_estim\n\nshadow = gen_class_shadow(circuit_oshot, psi0, 1000, num_qubits)\nexpval_estmt = build_estim_corrmat(coups, corrs, len(qbobs), shadow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This time, let us visualize the deviation observed between the exact\ncorrelation matrix ($C$) and the estimated correlation matrix\n($C^{\\prime}$) to assess the effectiveness of classical shadow\nformalism.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(4.2, 4))\nim = ax.imshow(expval_exact-expval_estmt, cmap=plt.get_cmap(\"RdBu\"), vmin=-1, vmax=1)\nax.xaxis.set_ticks(range(num_qubits))\nax.yaxis.set_ticks(range(num_qubits))\nax.xaxis.set_tick_params(labelsize=14)\nax.yaxis.set_tick_params(labelsize=14)\nax.set_title(\"Error in estimating the\\ncorrelation matrix\", fontsize=14)\n\nbar = fig.colorbar(im, pad=0.05, shrink=0.80)\nbar.set_label(r\"$\\Delta C_{ij}$\", fontsize=14, rotation=0)\nbar.ax.tick_params(labelsize=14)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Classical Machine Learning Models\n==========================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are multiple ways in which we can combine classical shadows and\nmachine learning. This could include training a model to learn the\nclassical representation of quantum systems based on some system\nparameter, estimating a property from such learned classical\nrepresentations, or a combination of both. In our case, we consider the\nproblem of using\n`kernel-based models <tutorial_kernel_based_training>`{.interpreted-text\nrole=\"doc\"} to learn the ground-state representation of the Heisenberg\nmodel Hamiltonian $H(x_l)$ from the coupling vector $x_l$, where\n$x_l = [J_{i,j} \\text{ for } i < j]$. The goal is to predict the\ncorrelation functions $C_{ij}$:\n\n$$\\big\\{x_l \\rightarrow \\sigma_T(\\rho(x_l)) \\rightarrow \\text{Tr}(\\hat{C}_{ij} \\sigma_T(\\rho(x_l))) \\big\\}_{l=1}^{N}.$$\n\nHere, we consider the following kernel-based machine learning model:\n\n$$\\hat{\\sigma}_{N} (x) = \\sum_{l=1}^{N} \\kappa(x, x_l)\\sigma_T (x_l) = \\sum_{l=1}^{N} \\left(\\sum_{l^{\\prime}=1}^{N} k(x, x_{l^{\\prime}})(K+\\lambda I)^{-1}_{l, l^{\\prime}} \\sigma_T(x_l) \\right),$$\n\nwhere $\\lambda > 0$ is a regularization parameter in cases when $K$ is\nnot invertible, $\\sigma_T(x_l)$ denotes the classical representation of\nthe ground state $\\rho(x_l)$ of the Heisenberg model constructed using\n$T$ randomized Pauli measurements, and $K_{ij}=k(x_i, x_j)$ is the\nkernel matrix with $k(x, x^{\\prime})$ as the kernel function.\n\nSimilarly, estimating an expectation value on the predicted ground state\n$\\sigma_T(x_l)$ using the trained model can then be done by evaluating:\n\n$$\\text{Tr}(\\hat{O} \\hat{\\sigma}_{N} (x)) = \\sum_{l=1}^{N} \\kappa(x, x_l)\\text{Tr}(O\\sigma_T (x_l)).$$\n\nWe train the classical kernel-based models using $N = 70$ randomly\nchosen values of the coupling matrices $J$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# imports for ML methods and techniques\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import svm\nfrom sklearn.kernel_ridge import KernelRidge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, to build the dataset, we use the function `build_dataset` that\ntakes as input the size of the dataset (`num_points`), the topology of\nthe lattice (`Nr` and `Nc`), and the number of randomized Pauli\nmeasurements ($T$) for the construction of classical shadows. The\n`X_data` is the set of coupling vectors that are defined as a stripped\nversion of the coupling matrix $J$, where only non-duplicate and\nnon-zero $J_{ij}$ are considered. The `y_exact` and `y_clean` are the\nset of correlation vectors, i.e., the flattened correlation matrix $C$,\ncomputed with respect to the ground-state obtained from exact\ndiagonalization and classical shadow representation (with $T=500$),\nrespectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def build_dataset(num_points, Nr, Nc, T=500):\n\n    num_qubits = Nr * Nc\n    X, y_exact, y_estim = [], [], []\n    coupling_mats = build_coupling_mats(num_points, Nr, Nc)\n\n    for coupling_mat in coupling_mats:\n        ham = Hamiltonian(coupling_mat)\n        eigvals, eigvecs = sp.sparse.linalg.eigs(ham.sparse_matrix())\n        psi = eigvecs[:, np.argmin(eigvals)]\n        shadow = gen_class_shadow(circuit_oshot, psi, T, num_qubits)\n\n        coups = list(it.product(range(num_qubits), repeat=2))\n        corrs = [corr_function(i, j) for i, j in coups]\n        qbobs = [x for sublist in corrs for x in sublist]\n\n        expval_exact = build_exact_corrmat(coups, corrs, circuit_exact, psi)\n        expval_estim = build_estim_corrmat(coups, corrs, len(qbobs), shadow)\n\n        coupling_vec = []\n        for coup in coupling_mat.reshape(1, -1)[0]:\n            if coup and coup not in coupling_vec:\n                coupling_vec.append(coup)\n        coupling_vec = np.array(coupling_vec) / np.linalg.norm(coupling_vec)\n\n        X.append(coupling_vec)\n        y_exact.append(expval_exact.reshape(1, -1)[0])\n        y_estim.append(expval_estim.reshape(1, -1)[0])\n\n    return np.array(X), np.array(y_exact), np.array(y_estim)\n\nX, y_exact, y_estim = build_dataset(100, Nr, Nc, 500)\nX_data, y_data = X, y_estim\nX_data.shape, y_data.shape, y_exact.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that our dataset is ready, we can shift our focus to the ML models.\nHere, we use two different Kernel functions: (i) Gaussian Kernel and\n(ii) Neural Tangent Kernel. For both of them, we consider the\nregularization parameter $\\lambda$ from the following set of values:\n\n$$\\lambda = \\left\\{ 0.0025, 0.0125, 0.025, 0.05, 0.125, 0.25, 0.5, 1.0, 5.0, 10.0 \\right\\}.$$\n\nNext, we define the kernel functions $k(x, x^{\\prime})$ for each of the\nmentioned kernels:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$k(x, x^{\\prime}) = e^{-\\gamma||x - x^{\\prime}||^{2}_{2}}. \\tag{Gaussian Kernel}$$\n\nFor the Gaussian kernel, the hyperparameter\n$\\gamma = N^{2}/\\sum_{i=1}^{N} \\sum_{j=1}^{N} ||x_i-x_j||^{2}_{2} > 0$\nis chosen to be the inverse of the average Euclidean distance $x_i$ and\n$x_j$. The kernel is implemented using the radial-basis function (rbf)\nkernel in the `sklearn` library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$k(x, x^{\\prime}) = k^{\\text{NTK}}(x, x^{\\prime}). \\tag{Neural Tangent Kernel}$$\n\nThe neural tangent kernel $k^{\\text{NTK}}$ used here is equivalent to an\ninfinite-width feed-forward neural network with four hidden layers and\nthat uses the rectified linear unit (ReLU) as the activation function.\nThis is implemented using the `neural_tangents` library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from neural_tangents import stax\ninit_fn, apply_fn, kernel_fn = stax.serial(\n    stax.Dense(32),\n    stax.Relu(),\n    stax.Dense(32),\n    stax.Relu(),\n    stax.Dense(32),\n    stax.Relu(),\n    stax.Dense(32),\n    stax.Relu(),\n    stax.Dense(1),\n)\nkernel_NN = kernel_fn(X_data, X_data, \"ntk\")\n\nfor i in range(len(kernel_NN)):\n    for j in range(len(kernel_NN)):\n        kernel_NN.at[i, j].set((kernel_NN[i][i] * kernel_NN[j][j]) ** 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the above two defined kernel methods, we obtain the best learning\nmodel by performing hyperparameter tuning using cross-validation for the\nprediction task of each $C_{ij}$. For this purpose, we implement the\nfunction `fit_predict_data`, which takes input as the correlation\nfunction index `cij`, kernel matrix `kernel`, and internal kernel\nmapping `opt` required by the kernel-based regression models from the\n`sklearn` library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n\ndef fit_predict_data(cij, kernel, opt=\"linear\"):\n\n    # training data (estimated from measurement data)\n    y = np.array([y_estim[i][cij] for i in range(len(X_data))])\n    X_train, X_test, y_train, y_test = train_test_split(\n        kernel, y, test_size=0.3, random_state=24\n    )\n\n    # testing data (exact expectation values)\n    y_clean = np.array([y_exact[i][cij] for i in range(len(X_data))])\n    _, _, _, y_test_clean = train_test_split(kernel, y_clean, test_size=0.3, random_state=24)\n\n    # hyperparameter tuning with cross validation\n    models = [\n        # Epsilon-Support Vector Regression\n        (lambda Cx: svm.SVR(kernel=opt, C=Cx, epsilon=0.1)),\n        # Kernel-Ridge based Regression\n        (lambda Cx: KernelRidge(kernel=opt, alpha=1 / (2 * Cx))),\n    ]\n\n    # Regularization parameter\n    hyperparams = [0.0025, 0.0125, 0.025, 0.05, 0.125, 0.25, 0.5, 1.0, 5.0, 10.0]\n    best_pred, best_cv_score, best_test_score = None, np.inf, np.inf\n    for model in models:\n        for hyperparam in hyperparams:\n            cv_score = -np.mean(\n                cross_val_score(\n                    model(hyperparam), X_train, y_train, cv=5,\n                    scoring=\"neg_root_mean_squared_error\",\n                )\n            )\n            if best_cv_score > cv_score:\n                best_model = model(hyperparam).fit(X_train, y_train)\n                best_pred = best_model.predict(X_test)\n                best_cv_score = cv_score\n                best_test_score = mean_squared_error(\n                    best_model.predict(X_test).ravel(), y_test_clean.ravel(), squared=False\n                )\n\n    return (\n        best_pred, y_test_clean, np.round(best_cv_score, 5), np.round(best_test_score, 5)\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We perform the fitting and prediction for each $C_{ij}$ and print the\noutput in a tabular format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kernel_list = [\"Gaussian kernel\", \"Neural Tangent kernel\"]\nkernel_data = np.zeros((num_qubits ** 2, len(kernel_list), 2))\ny_predclean, y_predicts1, y_predicts2 = [], [], []\n\nfor cij in range(num_qubits ** 2):\n    y_predict, y_clean, cv_score, test_score = fit_predict_data(cij, X_data, opt=\"rbf\")\n    y_predclean.append(y_clean)\n    kernel_data[cij][0] = (cv_score, test_score)\n    y_predicts1.append(y_predict)\n    y_predict, y_clean, cv_score, test_score = fit_predict_data(cij, kernel_NN)\n    kernel_data[cij][1] = (cv_score, test_score)\n    y_predicts2.append(y_predict)\n\n# For each C_ij print (best_cv_score, test_score) pair\nrow_format = \"{:>25}{:>35}{:>35}\"\nprint(row_format.format(\"Correlation\", *kernel_list))\nfor idx, data in enumerate(kernel_data):\n    print(\n        row_format.format(\n            f\"\\t C_{idx//num_qubits}{idx%num_qubits} \\t| \",\n            str(data[0]),\n            str(data[1]),\n        )\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Overall, we find that the models with the Gaussian kernel performed\nbetter than those with NTK for predicting the expectation value of the\ncorrelation function $C_{ij}$ for the ground state of the Heisenberg\nmodel. However, the best choice of $\\lambda$ differed substantially\nacross the different $C_{ij}$ for both kernels. We present the predicted\ncorrelation matrix $C^{\\prime}$ for randomly selected Heisenberg models\nfrom the test set below for comparison against the actual correlation\nmatrix $C$, which is obtained from the ground state found using exact\ndiagonalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 3, figsize=(14, 14))\ncorr_vals = [y_predclean, y_predicts1, y_predicts2]\nplt_plots = [1, 14, 25]\n\ncols = [\n    \"From {}\".format(col)\n    for col in [\"Exact Diagonalization\", \"Gaussian Kernel\", \"Neur. Tang. Kernel\"]\n]\nrows = [\"Model {}\".format(row) for row in plt_plots]\n\nfor ax, col in zip(axes[0], cols):\n    ax.set_title(col, fontsize=18)\n\nfor ax, row in zip(axes[:, 0], rows):\n    ax.set_ylabel(row, rotation=90, fontsize=24)\n\nfor itr in range(3):\n    for idx, corr_val in enumerate(corr_vals):\n        shw = axes[itr][idx].imshow(\n            np.array(corr_vals[idx]).T[plt_plots[itr]].reshape(Nr * Nc, Nr * Nc),\n            cmap=plt.get_cmap(\"RdBu\"), vmin=-1, vmax=1,\n        )\n        axes[itr][idx].xaxis.set_ticks(range(Nr * Nc))\n        axes[itr][idx].yaxis.set_ticks(range(Nr * Nc))\n        axes[itr][idx].xaxis.set_tick_params(labelsize=18)\n        axes[itr][idx].yaxis.set_tick_params(labelsize=18)\n\nfig.subplots_adjust(right=0.86)\ncbar_ax = fig.add_axes([0.90, 0.15, 0.015, 0.71])\nbar = fig.colorbar(shw, cax=cbar_ax)\n\nbar.set_label(r\"$C_{ij}$\", fontsize=18, rotation=0)\nbar.ax.tick_params(labelsize=16)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we also attempt to showcase the effect of the size of training\ndata $N$ and the number of Pauli measurements $T$. For this, we look at\nthe average root-mean-square error (RMSE) in prediction for each kernel\nover all two-point correlation functions $C_{ij}$. Here, the first plot\nlooks at the different training sizes $N$ with a fixed number of\nrandomized Pauli measurements $T=100$. In contrast, the second plot\nlooks at the different shadow sizes $T$ with a fixed training data size\n$N=70$. The performance improvement seems to be saturating after a\nsufficient increase in $N$ and $T$ values for all two kernels in both\nthe cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![image](/demonstrations/ml_classical_shadows/rmse_training.png){width=\"47.0%\"}\n\n![image](/demonstrations/ml_classical_shadows/rmse_shadow.png){width=\"47.0%\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conclusion\n==========\n\nThis demo illustrates how classical machine learning models can benefit\nfrom the classical shadow formalism for learning characteristics and\npredicting the behavior of quantum systems. As argued in Ref., this\nraises the possibility that models trained on experimental or quantum\ndata data can effectively address quantum many-body problems that cannot\nbe solved using classical methods alone.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References {#ml_classical_shadow_references}\n==========\n\nAbout the author\n================\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}