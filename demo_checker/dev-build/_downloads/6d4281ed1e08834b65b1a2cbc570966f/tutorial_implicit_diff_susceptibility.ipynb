{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implicit differentiation of variational quantum algorithms {#implicit_diff_susceptibility}\n==========================================================\n\n::: {.meta}\n:property=\\\"og:description\\\": Implicitly differentiating the the\nsolution of a VQA in PennyLane. :property=\\\"og:image\\\":\n<https://pennylane.ai/qml/_images/descartes.png>\n:::\n\n::: {.related}\ntutorial\\_backprop Quantum gradients with backpropagation\ntutorial\\_jax\\_transformations Using JAX with PennyLane\n:::\n\n*Authors: Shahnawaz Ahmed and Juan Felipe Carrasquilla \u00c1lvarez ---\nPosted: 28 November 2022. Last updated: 28 November 2022.*\n\nIn 1638, Ren\u00e9 Descartes, intrigued by (then amateur) Pierre de Fermat\\'s\nmethod of computing tangents, challenged Fermat to find the tangent to a\ncomplicated curve --- now called the folium of Descartes:\n\n$$x^3 + y^3 = 3axy.$$\n\n![](../demonstrations/implicit_diff/descartes.png){.align-center}\n\nWith its cubic terms, this curve represents an implicit equation which\ncannot be written as a simple expression $y = f(x)$. Therefore the task\nof calculating the tangent function seemed formidable for the method\nDescartes had then, except at the vertex. Fermat successfully provided\nthe tangents at not just the vertex but at any other point on the curve,\nbaffling Descartes and legitimizing the intellectual superiority of\nFermat. The technique used by Fermat was *implicit differentiation*. In\nthe above equation, we can begin by take derivatives on both sides and\nre-arrange the terms to obtain $dy/dx$.\n\n$$\\frac{dy}{dx} = -\\frac{x^2 - ay}{y^2 - ax}.$$\n\nImplicit differentiation can be used to compute gradients of such\nfunctions that cannot be written down explicitly using simple elementary\noperations. It is a basic technique of calculus that has recently found\nmany applications in machine learning --- from hyperparameter\noptimization to the training of neural ordinary differential equations\n(ODEs), and it has even led to the definition of a whole new class of\narchitectures, called Deep Equilibrium Models (DEQs).\n\nIntroduction\n------------\n\nThe idea of implicit differentiation can be applied in quantum physics\nto extend the power of automatic differentiation to situations where we\nare not able to explicitly write down the solution to a problem. As a\nconcrete example, consider a variational quantum algorithm (VQA) that\ncomputes the ground-state solution of a parameterized Hamiltonian $H(a)$\nusing a variational ansatz $|\\psi_{z}\\rangle$, where $z$ are the\nvariational parameters. This leads to the solution\n\n$$z^{*}(a) = \\arg\\,\\min_{z} \\langle \\psi_{z}|H(a)|\\psi_z\\rangle.$$\n\nAs we change $H(a)$, the solution also changes, but we do not obtain an\nexplicit function for $z^{*}(a)$. If we are interested in the properties\nof the solution state, we could measure the expectation values of some\noperator $A$ as\n\n$$\\langle A \\rangle (a) = \\langle \\psi_{z^{*}(a)}| A | \\psi_{z^{*}(a)}\\rangle.$$\n\nWith a VQA, we can find a solution to the optimization for a fixed\n$H(a)$. However, just like with the folium of Descartes, we do not have\nan explicit solution, so the gradient $\\partial_a \\langle A \\rangle (a)$\nis not easy to compute. The solution is only implicitly defined.\n\nAutomatic differentiation techniques that construct an explicit\ncomputational graph and differentiate through it by applying the chain\nrule for gradient computation cannot be applied here easily. A\nbrute-force application of automatic differentiation that finds\n$z^{*}(a)$ throughout the full optimization would require us to keep\ntrack of all intermediate variables and steps in the optimization and\ndifferentiate through them. This could quickly become computationally\nexpensive and memory-intensive for quantum algorithms. Implicit\ndifferentiation provides an alternative way to efficiently compute such\na gradient.\n\nSimilarly, there exist various other interesting quantities that can be\nwritten as gradients of a ground-state solution, e.g., nuclear forces in\nquantum chemistry, permanent electric dipolestatic polarizability, the\nstatic hyperpolarizabilities of various orders, fidelity\nsusceptibilities, and geometric tensors. All such quantities could\npossibly be computed using implicit differentiation on quantum devices.\nIn our recent work we present a unified way to implement such\ncomputations and other applications of implicit differentiation through\nvariational quantum algorithms.\n\nIn this demo we show how implicit gradients can be computed using a\nvariational algorithm written in *PennyLane* and powered by a modular\nimplicit differentiation implementation provided by the tool *JAXOpt*.\nWe compute the generalized susceptibility for a spin system by using a\nvariational ansatz to obtain a ground-state and implicitly\ndifferentiating through it. In order to compare the implicit solution,\nwe find the exact ground-state through eigendecomposition and determine\ngradients using automatic differentiation. Even though\neigendecomposition may become unfeasible for larger systems, for a small\nnumber of spins, it suffices for a comparison with our implicit\ndifferentiation approach.\n\nImplicit Differentiation\n------------------------\n\nWe consider the differentiation of a solution of the root-finding\nproblem, defined by\n\n$$f(z, a) = 0.$$\n\nA function $z^{*}(a)$ that satisfies $f(z^{*}(a), a) = 0$ gives a\nsolution map for fixed values of $a$. An explicit analytical solution\nis, however, difficult to obtain in general. This means that the direct\ndifferentiation of $\\partial_a z^{*}(a)$ is not always possible. Despite\nthat, some iterative algorithms may be able to compute the solution by\nstarting from an initial set of values for $z$, e.g., using a\nfixed-point solver. The optimality condition $f(z^{*}(a), a) = 0$ tells\nthe solver when a solution is found.\n\nImplicit differentiation can be used to compute $\\partial_a z^{*}(a)$\nmore efficiently than brute-force automatic differentiation, using only\nthe solution $z^{*}(a)$ and partial derivatives at the solution point.\nWe do not have to care about how the solution is obtained and,\ntherefore, do not need to differentiate through the solution-finding\nalgorithm.\n\nThe [Implicit function\ntheorem](https://en.wikipedia.org/wiki/Implicit_function_theorem) is a\nstatement about how the set of zeros of a system of equations is locally\ngiven by the graph of a function under some conditions. It can be\nextended to the complex domain and we state the theorem (informally)\nbelow.\n\n::: {.topic}\n**Implicit function theorem (IFT) (informal)**\n\nIf $f(z, a)$ is some analytic function where in a local neighbourhood\naround $(z_0, a_0)$ we have $f(z_0, a_0) = 0$, there exists an analytic\nsolution $z^{*}(a)$ that satisfies $f(z^{*}(a), a) = 0$.\n:::\n\n![](../demonstrations/implicit_diff/implicit_diff.png){.align-center}\n\nIn the figure above we can see solutions to the optimality condition\n$f(z, a) = 0 $ (red stars), which defines a curve $z^{*}(a)$. According\nto the IFT, the solution function is analytic, which means it can be\ndifferentiated at the solution points by simply differentiating the\nabove equation with respect to $a$, as\n\n$$\\partial_a f(z_0, a_0) + \\partial_{z} f(z_0, a_0) \\partial_{a} z^{*}(a) = 0,$$\n\nwhich leads to\n\n$$\\partial_{a} z^{*}(a) = - (\\partial_{z} f(z_0, a_0) )^{-1}\\partial_a f(z_0, a_0).$$\n\nThis shows that implicit differentiation can be used in situations where\nwe can phrase our optimization problem in terms of an optimality\ncondition or a fixed point equation that can be solved. In case of\noptimization tasks, such an optimality condition would be that, at the\nminima, the gradient of the cost function is zero --- i.e.,\n\n$$f(z, a) = \\partial_z g(z, a) = 0.$$\n\nThen, as long as we have the solution, $z^{*}(a)$, and the partial\nderivatives at the solution (in this case the Hessian of the cost\nfunction $g(z, a)$), $(\\partial_a f, \\partial_z f)$, we can compute\nimplicit gradients. Note that, for a multivariate function, the\ninversion $(\\partial_{z} f(z_0, a_0) )^{-1}$ needs to be defined and\neasy to compute. It is possible to approximate this inversion in a\nclever way by constructing a linear problem that can be solved\napproximately,.\n\nImplicit differentiation through a variational quantum algorithm\n----------------------------------------------------------------\n\n![](../demonstrations/implicit_diff/VQA.png){.align-center}\n\nWe now discuss how the idea of implicit differentiation can be applied\nto variational quantum algorithms. Let us take a parameterized\nHamiltonian $H(a)$, where $a$ is a parameter that can be continuously\nvaried. If $|\\psi_{z}\\rangle$ is a variational solution to the ground\nstate of $H(a)$, then we can find a $z^*(a)$ that minimizes the ground\nstate energy, i.e.,\n\n$$z^*(a) = \\arg\\, \\min_{z} \\langle \\psi_{z}| H(a) | \\psi_{z}\\rangle = \\arg \\min_{z} E(z, a),$$\n\nwhere $E(z, a)$ is the energy function. We consider the following\nHamiltonian\n\n$$H(a) = -J \\sum_{i} \\sigma^{z}_i \\sigma^{z}_{i+1} - \\gamma \\sum_{i} \\sigma^{x}_i + \\delta \\sum_{i} \\sigma^z_i - a A,$$\n\nwhere $J$ is the interaction, $\\sigma^{x}$ and $\\sigma^{z}$ are the\nspin-$\\frac{1}{2}$ operators and $\\gamma$ is the magnetic field strength\n(which is taken to be the same for all spins). The term\n$A = \\frac{1}{N}\\sum_i \\sigma^{z}_i$ is the magnetization and a small\nnon-zero magnetization $\\delta$ is added for numerical stability. We\nhave assumed a circular chain such that in the interaction term the last\nspin ($i = N-1$) interacts with the first ($i=0$).\n\nNow we could find the ground state of this Hamiltonian by simply taking\nthe eigendecomposition and applying automatic differentiation through\nthe eigendecomposition to compute gradients. We will compare this exact\ncomputation for a small system to the gradients given by implicit\ndifferentiation through a variationally obtained solution.\n\nWe define the following optimality condition at the solution point:\n\n$$f(z, a) = \\partial_z E(z, a) = 0.$$\n\nIn addition, if the conditions of the implicit function theorem are also\nsatisfied, i.e., $f$ is continuously differentiable with a non-singular\nJacobian at the solution, then we can apply the chain rule and determine\nthe implicit gradients easily.\n\nAt this stage, any other complicated function that depends on the\nvariational ground state can be easily differentiated using automatic\ndifferentiation by plugging in the value of $partial_a z^{*}(a)$ where\nit is required. The expectation value of the operator $A$ for the ground\nstate is\n\n$$\\langle A\\rangle = \\langle \\psi_{z^*}| A| \\psi_{z^*}\\rangle.$$\n\nIn the case where $A$ is just the energy, i.e., $A = H(a)$, the\nHellmann--Feynman theorem allows us to easily compute the gradient.\nHowever, for a general operator we need the gradients\n$\\partial_a z^{*}(a)$, which means that implicit differentiation is a\nvery elegant way to go beyond the Hellmann--Feynman theorem for\narbitrary expectation values.\n\nLet us now dive into the code and implementation.\n\n*Talk is cheap. Show me the code.* - Linus Torvalds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implicit differentiation of ground states in PennyLane\n======================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from functools import reduce\n\nfrom operator import add\n\nimport jax\nfrom jax import jit\nimport jax.numpy as jnp\nfrom jax.config import config\n\nimport pennylane as qml\nimport numpy as np\n\nimport jaxopt\n\nimport matplotlib.pyplot as plt\n\njax.config.update('jax_platform_name', 'cpu')\n\n# Use double precision numbers\nconfig.update(\"jax_enable_x64\", True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the Hamiltonian and measurement operator\n=================================================\n\nWe define the Hamiltonian by building the non-parametric part separately\nand adding the parametric part to it as a separate term for efficiency.\nNote that, for the example of generalized susceptibility, we are\nmeasuring expectation values of the operator $A$ that also defines the\nparametric part of the Hamiltonian. However, this is not necessary and\nwe could compute gradients for any other operator using implicit\ndifferentiation, as we have access to the gradients\n$\\partial_a z^{*}(a)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N = 4\nJ = 1.0\ngamma = 1.0\n\ndef build_H0(N, J, gamma):\n    \"\"\"Builds the non-parametric part of the Hamiltonian of a spin system.\n\n    Args:\n        N (int): Number of qubits/spins.\n        J (float): Interaction strength.\n        gamma (float): Interaction strength.\n\n    Returns:\n        qml.Hamiltonian: The Hamiltonian of the system.\n    \"\"\"\n    H = qml.Hamiltonian([], [])\n\n    for i in range(N - 1):\n        H += -J * qml.PauliZ(i) @ qml.PauliZ(i + 1)\n\n    H += -J * qml.PauliZ(N - 1) @ qml.PauliZ(0)\n\n    # Transverse\n    for i in range(N):\n        H += -gamma * qml.PauliX(i)\n\n    # Small magnetization for numerical stability\n    for i in range(N):\n        H += -1e-1 * qml.PauliZ(i)\n\n    return H\n\nH0 = build_H0(N, J, gamma)\nH0_matrix = qml.matrix(H0)\nA = reduce(add, ((1 / N) * qml.PauliZ(i) for i in range(N)))\nA_matrix = qml.matrix(A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Computing the exact ground state through eigendecomposition\n===========================================================\n\nWe now define a function that computes the exact ground state using\neigendecomposition. Ideally, we would like to take gradients of this\nfunction. It is possible to simply apply automatic differentiation\nthrough this exact ground-state computation. JAX has an implementation\nof differentiation through eigendecomposition.\n\nNote that we have some points in this plot that are `nan`, where the\ngradient computation through the eigendecomposition does not work. We\nwill see later that the computation through the VQA is more stable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@jit\ndef ground_state_solution_map_exact(a: float) -> jnp.array:\n    \"\"\"The ground state solution map that we want to differentiate\n    through, computed from an eigendecomposition.\n\n    Args:\n        a (float): The parameter in the Hamiltonian, H(a).\n\n    Returns:\n        jnp.array: The ground state solution for the H(a).\n    \"\"\"\n    H = H0_matrix + a * A_matrix\n    eval, eigenstates = jnp.linalg.eigh(H)\n    z_star = eigenstates[:, 0]\n    return z_star\n\n\na = jnp.array(np.random.uniform(0, 1.0))\nz_star_exact = ground_state_solution_map_exact(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Susceptibility computation through the ground state solution map\n================================================================\n\nLet us now compute the susceptibility function by taking gradients of\nthe expectation value of our operator $A$ w.r.t [a]{.title-ref}. We can\nuse [jax.vmap]{.title-ref} to vectorize the computation over different\nvalues of [a]{.title-ref}.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@jit\ndef expval_A_exact(a):\n    \"\"\"Expectation value of ``A`` as a function of ``a`` where we use the\n    ``ground_state_solution_map_exact`` function to find the ground state.\n\n    Args:\n        a (float): The parameter defining the Hamiltonian, H(a).\n\n    Returns:\n        float: The expectation value of A calculated using the variational state\n               that should be the ground state of H(a).\n    \"\"\"\n    z_star = ground_state_solution_map_exact(a)\n    eval = jnp.conj(z_star.T) @ A_matrix @ z_star\n    return eval.real\n\n# the susceptibility is the gradient of the expectation value\n_susceptibility_exact = jax.grad(expval_A_exact)\nsusceptibility_exact = jax.vmap(_susceptibility_exact)\n\nalist = jnp.linspace(0, 3, 1000)\nsusvals_exact = susceptibility_exact(alist)\n\nplt.plot(alist, susvals_exact)\nplt.xlabel(\"a\")\nplt.ylabel(r\"$\\partial_{a}\\langle A \\rangle$\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Computing susceptibility through implicit differentiation\n=========================================================\n\nWe use PennyLane to find a variational ground state for the Hamiltonian\n$H(a)$ and compute implicit gradients through the variational\noptimization procedure. We use the `jaxopt` library which contains an\nimplementation of gradient descent that automatically comes with\nimplicit differentiation capabilities. We are going to use that to\nobtain susceptibility by taking gradients through the ground-state\nminimization.\n\nDefining the variational state\n==============================\n\nIn PennyLane, we can implement a variational state in different ways, by\ndefining a quantum circuit. There are also useful template circuits\navailable, such as `~pennylane.SimplifiedTwoDesign`{.interpreted-text\nrole=\"class\"}, which implements the\n`two-design ansatz <tutorial_unitary_designs>`{.interpreted-text\nrole=\"doc\"}. The ansatz consists of layers of Pauli-Y rotations with\ncontrolled-Z gates. In each layer there are `N - 1` parameters for the\nPauli-Y gates. Therefore, the ansatz is efficient as long as we have\nenough layers for it so that is expressive enough to represent the\nground-state.\n\nWe set `n_layers = 5`, but you can redo this example with fewer layers\nto see how a less expressive ansatz leads to error in the susceptibility\ncomputation.\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nThe setting `shots=None` makes for the computation of gradients using\nreverse-mode autodifferentiation (backpropagation). It allows us to\njust-in-time (JIT) compile the functions that compute expectation values\nand gradients. In a real device we would use a finite number of shots\nand the gradients would be computed using the parameter-shift rule.\nHowever, this may be slower.\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "variational_ansatz = qml.SimplifiedTwoDesign\nn_layers = 5\nweights_shape = variational_ansatz.shape(n_layers, N)\n\ndev = qml.device(\"default.qubit.jax\", wires=N, shots=None)\n\n@jax.jit\n@qml.qnode(dev, interface=\"jax\")\ndef energy(z, a):\n    \"\"\"Computes the energy for a Hamiltonian H(a) using a measurement on the\n    variational state U(z)|0> with U(z) being any circuit ansatz.\n\n    Args:\n        z (jnp.array): The variational parameters for the ansatz (circuit)\n        a (jnp.array): The Hamiltonian parameters.\n\n    Returns:\n        float: The expectation value (energy).\n    \"\"\"\n    variational_ansatz(*z, wires=range(N))\n    # here we compute the Hamiltonian coefficients and operations\n    # 'by hand' because the qml.Hamiltonian class does not support\n    # operator arithmetic with JAX device arrays.\n    coeffs = jnp.concatenate([H0.coeffs, a * A.coeffs])\n    return qml.expval(qml.Hamiltonian(coeffs, H0.ops + A.ops))\n\n\nz_init = [jnp.array(2 * np.pi * np.random.random(s)) for s in weights_shape]\na = jnp.array([0.5])\nprint(\"Energy\", energy(z_init, a))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Computing ground states using a variational quantum algorithm (VQA)\n===================================================================\n\nWe construct a loss function that defines a ground-state minimization\ntask. We are looking for variational parameters `z` that minimize the\nenergy function. Once we find a set of parameters `z`, we wish to\ncompute the gradient of any function of the ground state w.r.t. `a`.\n\nFor the implicit differentiation we will use the tool `jaxopt`, which\nimplements modular implicit differentiation for various cases; e.g., for\nfixed-point functions or optimization. We can directly use `jaxopt` to\noptimize our loss function and then compute implicit gradients through\nit. It all works due to\n`PennyLane's integration with JAX <tutorial_jax_transformations>`{.interpreted-text\nrole=\"doc\"}.\n\nThe implicit differentiation formulas can even be [implemented manually\nwith\nJAX](https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html#implicit-function-differentiation-of-iterative-implementations).\nThese formulas are implemented in a modular way, using the\n`jaxopt.GradientDescent` optimizer with `implicit_diff=True`. We use the\nseamless integration between PennyLane, JAX and JAXOpt to compute the\nsusceptibility.\n\nSince everything is written in JAX, simply calling the `jax.grad`\nfunction works as `jaxopt` computes the implicit gradients and plugs it\nany computation used by `jax.grad`. We can also just-in-time (JIT)\ncompile all functions although the compilation may take some time as the\nnumber of spins or variational ansatz becomes more complicated. Once\ncompiled, all computes run very fast for any parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def ground_state_solution_map_variational(a, z_init):\n    \"\"\"The ground state solution map that we want to differentiate\n    through.\n\n    Args:\n        a (float): The parameter in the Hamiltonian, H(a).\n        z_init [jnp.array(jnp.float)]: The initial guess for the variational\n                                       parameters.\n\n    Returns:\n        z_star (jnp.array [jnp.float]): The parameters that define the\n                                        ground-state solution.\n    \"\"\"\n    @jax.jit\n    def loss(z, a):\n        \"\"\"Loss function for the ground-state minimization with regularization.\n\n        Args:\n            z (jnp.array): The variational parameters for the ansatz (circuit)\n            a (jnp.array): The Hamiltonian parameters.\n\n        Returns:\n            float: The loss value (energy + regularization)\n        \"\"\"\n        return (\n            energy(z, a) + 0.001 * jnp.sum(jnp.abs(z[0])) + 0.001 * jnp.sum(jnp.abs(z[1]))\n        )\n    gd = jaxopt.GradientDescent(\n        fun=loss,\n        stepsize=1e-2,\n        acceleration=True,\n        maxiter=1000,\n        implicit_diff=True,\n        tol=1e-15,\n    )\n    z_star = gd.run(z_init, a=a).params\n    return z_star\n\na = jnp.array(np.random.uniform(0, 1.0))  # A random ``a``\nz_star_variational = ground_state_solution_map_variational(a, z_init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Computing gradients through the VQA simply by calling `jax.grad`\n================================================================\n\nWe can compute the susceptibility values by simply using `jax.grad`.\nAfter the first call, the function is compiled and subsequent calls\nbecome much faster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@jax.jit\n@qml.qnode(dev, interface=\"jax\")\ndef expval_A_variational(z: float) -> float:\n    \"\"\"Expectation value of $A$ as a function of $a$ where we use the\n    a variational ground state solution map.\n\n    Args:\n        a (float): The parameter in the Hamiltonian, H(a).\n\n    Returns:\n        float: The expectation value of M on the ground state of H(a)\n    \"\"\"\n    variational_ansatz(*z, wires=range(N))\n    return qml.expval(A)\n\n@jax.jit\ndef groundstate_expval_variational(a, z_init) -> float:\n    \"\"\"Computes ground state and calculates the expectation value of the operator M.\n\n    Args:\n        a (float): The parameter in the Hamiltonian, H(a).\n        z_init [jnp.array(jnp.float)]: The initial guess for the variational parameters.\n        H0 (qml.Hamiltonian): The static part of the Hamiltonian\n    \"\"\"\n    z_star = ground_state_solution_map_variational(a, z_init)\n    return expval_A_variational(z_star)\n\nsusceptibility_variational = jax.jit(jax.grad(groundstate_expval_variational, argnums=0))\nz_init = [jnp.array(2 * np.pi * np.random.random(s)) for s in weights_shape]\nprint(\"Susceptibility\", susceptibility_variational(alist[0], z_init))\n\nsusvals_variational = []\n\nfor i in range(len(alist)):\n    susvals_variational.append(susceptibility_variational(alist[i], z_init))\n\nplt.plot(alist, susvals_variational, label=\"Implicit diff through VQA\")\nplt.plot(alist, susvals_exact, \"--\", c=\"k\", label=\"Automatic diff through eigendecomposition\")\nplt.xlabel(\"a\")\nplt.ylabel(r\"$\\partial_{a}\\langle A \\rangle$\")\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PennyLane version and details\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(qml.about())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conclusion\n==========\n\nWe have shown how a combination of JAX, PennyLane and JAXOpt can be used\nto compute implicit gradients through a VQA. The ability to compute such\ngradients opens up new possibilities, e.g., the design of a Hamiltonian\nsuch that its ground-state has certain properties. It is also possible\nto perhaps look at this inverse-design of the Hamiltonian as a control\nproblem. Implicit differentiation in the classical setting allows\ndefining a new type of neural network layer --- implicit layers such as\nneural ODEs. In a similar way, we hope this demo the inspires creation\nof new architectures for quantum neural networks, perhaps a quantum\nversion of neural ODEs or quantum implicit layers.\n\nIn future works, it would be important to assess the cost of running\nimplicit differentiation through an actual quantum computer and\ndetermine the quality of such gradients as a function of noise as\nexplored in a related recent work [^1].\n\nReferences\n==========\n\nAbout the authors\n=================\n\n[^1]: Olivia Di Matteo, R. M. Woloshyn \\\"Quantum computing fidelity\n    susceptibility using automatic differentiation\\\"\n    [arXiv:2207.06526](https://arxiv.org/abs/2207.06526), 2022.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}