{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Introduction to Geometric Quantum Machine Learning\n==================================================\n\n::: {.meta}\n:property=\\\"og:description\\\": Using the natural symmetries in a quantum\nlearning problem can improve learning :property=\\\"og:image\\\":\n<https://pennylane.ai/qml/_images/equivariant_thumbnail.jpeg>\n:::\n\n*Author: Richard East --- Posted: 18 October 2022.*\n\nIntroduction\n------------\n\nSymmetries are at the heart of physics. Indeed in condensed matter and\nparticle physics we often define a thing simply by the symmetries it\nadheres to. What does symmetry mean for those in machine learning? In\nthis context the ambition is straightforward --- it is a means to reduce\nthe parameter space and improve the trained model\\'s ability to\nsucessfully label unseen data, i.e., its ability to generalise.\n\nSuppose we have a learning task and the data we are learning from has an\nunderlying symmetry. For example, consider a game of Noughts and Crosses\n(aka Tic-tac-toe): if we win a game, we would have won it if the board\nwas rotated or flipped along any of the lines of symmetry. Now if we\nwant to train an algorithm to spot the outcome of these games, we can\neither ignore the existence of this symmetry or we can somehow include\nit. The advantage of paying attention to the symmetry is it identifies\nmultiple configurations of the board as \\'the same thing\\' as far as the\nsymmetry is concerned. This means we can reduce our parameter space, and\nso the amount of data our algorithm must sift through is immediately\nreduced. Along the way, the fact that our learning model must encode a\nsymmetry that actually exists in the system we are trying to represent\nnaturally encourages our results to be more generalisable. The encoding\nof symmetries into our learning models is where the term *equivariance*\nwill appear. We will see that demanding that certain symmetries are\nincluded in our models means that the mappings that make up our\nalgorithms must be such that we could transform our input data with\nrespect to a certain symmetry, then apply our mappings, and this would\nbe the same as applying the mappings and then transforming the output\ndata with the same symmetry. This is the technical property that gives\nus the name \\\"equavariant learning\\\".\n\nIn classical machine learning, this area is often referred to as\ngeometric deep learning (GDL) due to the traditional association of\nsymmetry to the world of geometry, and the fact that these\nconsiderations usually focus on deep neural networks (see or for a broad\nintroduction). We will refer to the quantum computing version of this as\n*quantum geometric machine learning* (QGML).\n\nRepresentation theory in circuits\n---------------------------------\n\nThe first thing to discuss is how do we work with symmetries in the\nfirst place? The answer lies in the world of group representation\ntheory.\n\nFirst, let\\'s define what we mean by a group:\n\n**Definition**: A group is a set $G$ together with a binary operation on\n$G$, here denoted $\\circ$, that combines any two elements $a$ and $b$ to\nform an element of $G$, denoted $a \\circ b$, such that the following\nthree requirements, known as group axioms, are satisfied as follows:\n\n1.  **Associativity**: For all $a, b, c$ in $G$, one has\n    $(a \\circ b) \\circ c=a \\circ (b \\circ c)$.\n\n2.  \n\n    **Identity element**: There exists an element $e$ in $G$ such that, for every $a$ in $G$, one\n\n    :   has $e \\circ a=a$ and $a \\circ e=a$. Such an element is unique.\n        It is called the identity element of the group.\n\n3.  \n\n    **Inverse element**: For each $a$ in $G$, there exists an element $b$ in $G$\n\n    :   such that $a \\circ b=e$ and $b \\circ a=e$, where $e$ is the\n        identity element. For each $a$, the element $b$ is unique: it is\n        called the inverse of $a$ and is commonly denoted $a^{-1}$.\n\nWith groups defined, we are in a position to articulate what a\nrepresentation is: Let $\\varphi$ be a map sending $g$ in group $G$ to a\nlinear map $\\varphi(g): V \\rightarrow V$, for some vector space $V$,\nwhich satisfies\n\n$$\\varphi\\left(g_{1} g_{2}\\right)=\\varphi\\left(g_{1}\\right) \\circ \\varphi\\left(g_{2}\\right) \\quad \\text { for all } g_{1}, g_{2} \\in G.$$\n\nThe idea here is that just as elements in a group act on each other to\nreach further elements, i.e., $g\\circ h = k$, a representation sends us\nto a mapping acting on a vector space such that\n$\\varphi(g)\\circ \\varphi(h) = \\varphi(k)$. In this way we are\nrepresenting the structure of the group as a linear map. For a\nrepresentation, our mapping must send us to the general linear group\n$GL(n)$ (the space of invertible $n \\times n$ matrices with matrix\nmultiplication as the group multiplication). Note how this is both a\ngroup, and by virtue of being a collection of invertible matrices, also\na set of linear maps (they\\'re all invertble matrices that can act on\nrow vectors). Fundamentally, representation theory is based on the\nprosaic observation that linear algebra is easy and group theory is\nabstract. So what if we can study groups via linear maps?\n\nNow due to the importance of unitarity in quantum mechnics, we are\nparticularly interested in the unitary representations: representations\nwhere the linear maps are unitary matrices. If we can identify these\nthen we will have a way to naturally encode groups in quantum circuits\n(which are mostly made up of unitary gates).\n\n![](../demonstrations/geometric_qml/sphere_equivariant.png){.align-center\nwidth=\"45.0%\"}\n\nHow does all this relate to symmetries? Well, a large class of\nsymmetries can be characterised as a group, where all the elements of\nthe group leave some space we are considering unchanged. Let\\'s consider\nan example: the symmetries of a sphere. Now when we think of this\nsymmetry we probably think something along the lines of \\\"it\\'s the same\nno matter how we rotate it, or flip it left to right, etc\\\". There is\nthis idea of being invariant under some operation. We also have the idea\nof being able to undo these actions: if we rotate one way, we can rotate\nit back. If we flip the sphere right-to-left we can flip it\nleft-to-right to get back to where we started (notice too all these\ninverses are unique). Trivially we can also do nothing. What exactly are\nwe describing here? We have elements that correspond to an action on a\nsphere that can be inverted and for which there exists an identity. It\nis also trivially the case here that if we consider three operations a,\nb, c from the set of rotations and reflections of the sphere, that if we\ncombine two of them together then\n$a\\circ (b \\circ c) = (a\\circ b) \\circ c$. The operations are\nassociative. These features turn out to literally define a group!\n\nAs we\\'ve seen the group in itself is a very abstract creature; this is\nwhy we look to its representations. The group labels what symmetries we\ncare about, they tell us the mappings that our system is invariant\nunder, and the unitary representations show us how those symmetries look\non a particular space of unitary matrices. If we want to encode the\nstructure of the symmeteries in a quantum circuit we must restrict our\ngates to being unitary representations of the group.\n\nThere remains one question: *what is equivariance?* With our newfound\nknowledge of group representation theory we are ready to tackle this.\nLet $G$ be our group, and $V$ and $W$, with elements $v$ and $w$\nrespectively, be vector spaces over some field $F$ with a map $f$\nbetween them. Suppose we have representations\n$\\varphi: G \\rightarrow GL(V)$ and $\\psi: G \\rightarrow GL(W)$.\nFurthermore, let\\'s write $\\varphi_g$ for the representation of $g$ as a\nlinear map on $V$ and $\\psi_g$ as the same group element represented as\na linear map on $W$ respectively. We call $f$ *equivariant* if\n\n$$f(\\varphi_g(v))=\\psi_g(f(v)) \\quad \\text { for all } g\\in G.$$\n\nThe importance of such a map in machine learning is that if, for\nexample, our neural network layers are equivariant maps then two inputs\nthat are related by some intrinsic symmetry (maybe they are reflections)\npreserve this information in the outputs.\n\nConsider the following figure for example. What we see is a board with a\ncross in a certain square on the left and some numerical encoding of\nthis on the right, where the 1 is where the X is in the number grid. We\npresent an equivariant mapping between these two spaces with respect to\na group action that is a rotation or a swap (here a $\\pi$ rotation). We\ncan either apply a group action to the original grid and then map to the\nnumber grid, or we could map to the number grid and then apply the group\naction. Equivariance demands that the result of either of these\nprocedures should be the same.\n\n![](../demonstrations/geometric_qml/equivariant-example.jpg){.align-center\nwidth=\"80.0%\"}\n\nGiven the vast amount of input data required to train a neural network\nthe principle that one can pre-encode known symmetry structures into the\nnetwork allows us to learn better and faster. Indeed it is the reason\nfor the success of convolutional neural networks (CNNs) for image\nanalysis, where it is known they are equivariant with respect to\ntranslations. They naturally encode the idea that a picture of a dog is\nsymmetrically related to the same picture slid to the left by n pixels,\nand they do this by having neural network layers that are equivariant\nmaps. With our focus on unitary representations (and so quantum\ncircuits) we are looking to extend this idea to quantum machine\nlearning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Noughts and Crosses\n===================\n\nLet\\'s look at the game of noughts and crosses, as inspired by. Two\nplayers take turns to place a O or an X, depending on which player they\nare, in a 3x3 grid. The aim is to get three of your symbols in a row,\ncolumn, or diagonal. As this is not always possible depending on the\nchoices of the players, there could be a draw. Our learning task is to\ntake a set of completed games labelled with their outcomes and teach the\nalgorithm to identify these correctly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This board of nine elements has the symmetry of the square, also known\nas the *dihedral group*. This means it is symmetric under\n$\\frac{\\pi}{2}$ rotations and flips about the lines of symmetry of a\nsquare (vertical, horizontal, and both diagonals).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](../demonstrations/geometric_qml/NandC_sym.png){.align-center\nwidth=\"70.0%\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The question is, how do we encode this in our QML problem?**\n\nFirst, let us encode this problem classically. We will consider a\nnine-element vector $v$, each element of which identifies a square of\nthe board. The entries themselves can be $+1$,$0$,$-1,$ representing a\nnought, no symbol, or a cross. The label is one-hot encoded in a vector\n$y=(y_O,y_- , y_X)$ with $+1$ in the correct label and $-1$ in the\nothers. For instance (-1,-1,1) would represent an X in the relevant\nposition.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To create the quantum model let us take nine qubits and let them\nrepresent squares of our board. We\\'ll initialise them all as\n$|0\\rangle$, which we note leaves the board invariant under the\nsymmetries of the problem (flip and rotate all you want, it\\'s still\ngoing to be zeroes whatever your mapping). We will then look to apply\nsingle qubit $R_x(\\theta)$ rotations on individual qubits, encoding each\nof the possibilities in the board squares at an angle of\n$\\frac{2\\pi}{3}$ from each other. For our parameterised gates we will\nhave a single-qubit $R_x(\\theta_1)$ and $R_y(\\theta_2)$ rotation at each\npoint. We will then use $CR_y(\\theta_3)$ for two-qubit entangling gates.\nThis implies that, for each encoding, crudely, we\\'ll need 18\nsingle-qubit rotation parameters and $\\binom{9}{2}=36$ two-qubit gate\nrotations. Let\\'s see how, by using symmetries, we can reduce this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![..](../demonstrations/geometric_qml/grid.jpg){.align-center\nwidth=\"35.0%\"}\n\nThe indexing of our game board.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The secret will be to encode the symmetries into the gate set so the\nobservables we are interested in inherently respect the symmetries. How\ndo we do this? We need to select the collections of gates that commute\nwith the symmetries. In general, we can use the twirling formula for\nthis:\n\n::: {.tip}\n::: {.title}\nTip\n:::\n\nLet $\\mathcal{S}$ be the group that encodes our symmetries and $U$ be a\nunitary representation of $\\mathcal{S}$. Then,\n\n$$\\mathcal{T}_{U}[X]=\\frac{1}{|\\mathcal{S}|} \\sum_{s \\in \\mathcal{S}} U(s) X U(s)^{\\dagger}$$\n\ndefines a projector onto the set of operators commuting with all\nelements of the representation, i.e.,\n$\\left[\\mathcal{T}_{U}[X], U(s)\\right]=$ 0 for all $X$ and\n$s \\in \\mathcal{S}$.\n:::\n\nThe twirling process applied to an arbitrary unitary will give us a new\nunitary that commutes with the group as we require. We remember that\nunitary gates typically have the form $W = \\exp(-i\\theta H)$, where $H$\nis a Hermitian matrix called a *generator*, and $\\theta$ may be fixed or\nleft as a free parameter. A recipe for creating a unitary that commutes\nwith our symmetries is to *twirl the generator of the gate*, i.e., we\nmove from the gate $W = \\exp(-i\\theta H)$ to the gate\n$W' = \\exp(-i\\theta\\mathcal{T}_U[H])$. When each term in the twirling\nformula acts on different qubits, then this unitary would further\nsimplify to\n\n$$W' = \\bigotimes_{s\\in\\mathcal{S}}U(s)\\exp(-i\\tfrac{\\theta}{\\vert\\mathcal{S}\\vert})U(s)^\\dagger.$$\n\nFor simplicity, we can absorb the normalization factor\n$\\vert\\mathcal{S}\\vert$ into the free parameter $\\theta$.\n\nSo let\\'s look again at our choice of gates: single-qubit $R_x(\\theta)$\nand $R_y(\\theta)$ rotations, and entangling two-qubit $CR_y(\\phi)$\ngates. What will we get by twirling these?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this particular instance we can see the action of the twirling\noperation geometrically as the symmetries involved are all permutations.\nLet\\'s consider the $R_x$ rotation acting on one qubit. Now if this\nqubit is in the centre location on the grid, then we can flip around any\nsymmetry axis we like, and this operation leaves the qubit invariant, so\nwe\\'ve identified one equivariant gate immediately. If the qubit is on\nthe corners, then the flipping will send this qubit rotation to each of\nthe other corners. Similarly, if a qubit is on the central edge then the\nrotation gate will be sent round the other edges. So we can see that the\ntwirling operation is a sum over all the possible outcomes of performing\nthe symmetry action (the sum over the symmetry group actions). Having\ndone this we can see that for a single-qubit rotation the invariant maps\nare rotations on the central qubit, at all the corners, and at all the\ncentral edges (when their rotation angles are fixed to be the same).\n\nAs an example consider the following figure, where we take a $R_x$ gate\nin the corner and then apply all the symmetries of a square. The result\nof this twirling leads us to have the same gate at all the corners.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](../demonstrations/geometric_qml/twirl.jpeg){.align-center\nwidth=\"70.0%\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For entangling gates the situation is similar. There are three invariant\nclasses, the centre entangled with all corners, with all edges, and the\nedges paired in a ring.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The prediction of a label is obtained via a one-hot-encoding by\nmeasuring the expectation values of three invariant observables:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$O_{-}=Z_{\\text {middle }}=Z_{4}$$\n\n$$O_{\\circ}=\\frac{1}{4} \\sum_{i \\in \\text { corners }} Z_{i}=\\frac{1}{4}\\left[Z_{0}+Z_{2}+Z_{6}+Z_{8}\\right]$$\n\n$$O_{\\times}=\\frac{1}{4} \\sum_{i \\in \\text { edges }} Z_{i}=\\frac{1}{4}\\left[Z_{1}+Z_{3}+Z_{5}+Z_{7}\\right]$$\n\n$$\\hat{\\boldsymbol{y}}=\\left(\\left\\langle O_{\\circ}\\right\\rangle,\\left\\langle O_{-}\\right\\rangle,\\left\\langle O_{\\times}\\right\\rangle\\right)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the quantum encoding of the symmetries into a learning problem.\nA prediction for a given data point will be obtained by selecting the\nclass for which the observed expectation value is the largest.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have a specific encoding and have decided on our observables\nwe need to choose a suitable cost function to optimise. We will use an\n$l_2$ loss function acting on pairs of games and labels $D={(g,y)}$,\nwhere $D$ is our dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s now implement this!\n\nFirst let\\'s generate some games. Here we are creating a small program\nthat will play Noughts and Crosses against itself in a random fashion.\nOn completion, it spits out the winner and the winning board, with\nnoughts as +1, draw as 0, and crosses as -1. There are 26,830 different\npossible games but we will only sample a few hundred.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport random\n\n# Fix seeds for reproducability\ntorch.backends.cudnn.deterministic = True\ntorch.manual_seed(16)\nrandom.seed(16)\n\n#  create an empty board\ndef create_board():\n    return torch.tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n\n\n# Check for empty places on board\ndef possibilities(board):\n    l = []\n    for i in range(len(board)):\n        for j in range(3):\n            if board[i, j] == 0:\n                l.append((i, j))\n    return l\n\n\n# Select a random place for the player\ndef random_place(board, player):\n    selection = possibilities(board)\n    current_loc = random.choice(selection)\n    board[current_loc] = player\n    return board\n\n\n# Check if there is a winner by having 3 in a row\ndef row_win(board, player):\n    for x in range(3):\n        lista = []\n        win = True\n\n        for y in range(3):\n            lista.append(board[x, y])\n\n            if board[x, y] != player:\n                win = False\n\n        if win:\n            break\n\n    return win\n\n\n# Check if there is a winner by having 3 in a column\ndef col_win(board, player):\n    for x in range(3):\n        win = True\n\n        for y in range(3):\n            if board[y, x] != player:\n                win = False\n\n        if win:\n            break\n\n    return win\n\n\n# Check if there is a winner by having 3 along a diagonal\ndef diag_win(board, player):\n    win1 = True\n    win2 = True\n    for x, y in [(0, 0), (1, 1), (2, 2)]:\n        if board[x, y] != player:\n            win1 = False\n\n    for x, y in [(0, 2), (1, 1), (2, 0)]:\n        if board[x, y] != player:\n            win2 = False\n\n    return win1 or win2\n\n\n# Check if the win conditions have been met or if a draw has occurred\ndef evaluate_game(board):\n    winner = None\n    for player in [1, -1]:\n        if row_win(board, player) or col_win(board, player) or diag_win(board, player):\n            winner = player\n\n    if torch.all(board != 0) and winner == None:\n        winner = 0\n\n    return winner\n\n\n# Main function to start the game\ndef play_game():\n    board, winner, counter = create_board(), None, 1\n    while winner == None:\n        for player in [1, -1]:\n            board = random_place(board, player)\n            counter += 1\n            winner = evaluate_game(board)\n            if winner != None:\n                break\n\n    return [board.flatten(), winner]\n\n\ndef create_dataset(size_for_each_winner):\n    game_d = {-1: [], 0: [], 1: []}\n\n    while min([len(v) for k, v in game_d.items()]) < size_for_each_winner:\n        board, winner = play_game()\n        if len(game_d[winner]) < size_for_each_winner:\n            game_d[winner].append(board)\n\n    res = []\n    for winner, boards in game_d.items():\n        res += [(board, winner) for board in boards]\n\n    return res\n\n\nNUM_TRAINING = 450\nNUM_VALIDATION = 600\n\n# Create datasets but with even numbers of each outcome\nwith torch.no_grad():\n    dataset = create_dataset(NUM_TRAINING // 3)\n    dataset_val = create_dataset(NUM_VALIDATION // 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let\\'s create the relevant circuit expectation values that respect\nthe symmetry classes we defined over the single-site and two-site\nmeasurements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\nimport matplotlib.pyplot as plt\n\n# Set up a nine-qubit system\ndev = qml.device(\"default.qubit.torch\", wires=9)\n\nob_center = qml.PauliZ(4)\nob_corner = (qml.PauliZ(0) + qml.PauliZ(2) + qml.PauliZ(6) + qml.PauliZ(8)) * (1 / 4)\nob_edge = (qml.PauliZ(1) + qml.PauliZ(3) + qml.PauliZ(5) + qml.PauliZ(7)) * (1 / 4)\n\n# Now let's encode the data in the following qubit models, first with symmetry\n@qml.qnode(dev, interface=\"torch\")\ndef circuit(x, p):\n\n    qml.RX(x[0], wires=0)\n    qml.RX(x[1], wires=1)\n    qml.RX(x[2], wires=2)\n    qml.RX(x[3], wires=3)\n    qml.RX(x[4], wires=4)\n    qml.RX(x[5], wires=5)\n    qml.RX(x[6], wires=6)\n    qml.RX(x[7], wires=7)\n    qml.RX(x[8], wires=8)\n\n    # Centre single-qubit rotation\n    qml.RX(p[0], wires=4)\n    qml.RY(p[1], wires=4)\n\n    # Corner single-qubit rotation\n    qml.RX(p[2], wires=0)\n    qml.RX(p[2], wires=2)\n    qml.RX(p[2], wires=6)\n    qml.RX(p[2], wires=8)\n\n    qml.RY(p[3], wires=0)\n    qml.RY(p[3], wires=2)\n    qml.RY(p[3], wires=6)\n    qml.RY(p[3], wires=8)\n\n    # Edge single-qubit rotation\n    qml.RX(p[4], wires=1)\n    qml.RX(p[4], wires=3)\n    qml.RX(p[4], wires=5)\n    qml.RX(p[4], wires=7)\n\n    qml.RY(p[5], wires=1)\n    qml.RY(p[5], wires=3)\n    qml.RY(p[5], wires=5)\n    qml.RY(p[5], wires=7)\n\n    # Entagling two-qubit gates\n    # circling the edge of the board\n    qml.CRY(p[6], wires=[0, 1])\n    qml.CRY(p[6], wires=[2, 1])\n    qml.CRY(p[6], wires=[2, 5])\n    qml.CRY(p[6], wires=[8, 5])\n    qml.CRY(p[6], wires=[8, 7])\n    qml.CRY(p[6], wires=[6, 7])\n    qml.CRY(p[6], wires=[6, 3])\n    qml.CRY(p[6], wires=[0, 3])\n\n    # To the corners from the centre\n    qml.CRY(p[7], wires=[4, 0])\n    qml.CRY(p[7], wires=[4, 2])\n    qml.CRY(p[7], wires=[4, 6])\n    qml.CRY(p[7], wires=[4, 8])\n\n    # To the centre from the edges\n    qml.CRY(p[8], wires=[1, 4])\n    qml.CRY(p[8], wires=[3, 4])\n    qml.CRY(p[8], wires=[5, 4])\n    qml.CRY(p[8], wires=[7, 4])\n\n    return [qml.expval(ob_center), qml.expval(ob_corner), qml.expval(ob_edge)]\n\n\nfig, ax = qml.draw_mpl(circuit)([0] * 9, 18 * [0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s also look at the same series of gates but this time they are\napplied independently from one another, so we won\\'t be preserving the\nsymmetries with our gate operations. Practically this also means more\nparameters, as previously groups of gates were updated together.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev, interface=\"torch\")\ndef circuit_no_sym(x, p):\n\n    qml.RX(x[0], wires=0)\n    qml.RX(x[1], wires=1)\n    qml.RX(x[2], wires=2)\n    qml.RX(x[3], wires=3)\n    qml.RX(x[4], wires=4)\n    qml.RX(x[5], wires=5)\n    qml.RX(x[6], wires=6)\n    qml.RX(x[7], wires=7)\n    qml.RX(x[8], wires=8)\n\n    # Centre single-qubit rotation\n    qml.RX(p[0], wires=4)\n    qml.RY(p[1], wires=4)\n\n    # Note in this circuit the parameters aren't all the same.\n    # Previously they were identical to ensure they were applied\n    # as one combined gate. The fact they can all vary independently\n    # here means we aren't respecting the symmetry.\n\n    # Corner single-qubit rotation\n    qml.RX(p[2], wires=0)\n    qml.RX(p[3], wires=2)\n    qml.RX(p[4], wires=6)\n    qml.RX(p[5], wires=8)\n\n    qml.RY(p[6], wires=0)\n    qml.RY(p[7], wires=2)\n    qml.RY(p[8], wires=6)\n    qml.RY(p[9], wires=8)\n\n    # Edge single-qubit rotation\n    qml.RX(p[10], wires=1)\n    qml.RX(p[11], wires=3)\n    qml.RX(p[12], wires=5)\n    qml.RX(p[13], wires=7)\n\n    qml.RY(p[14], wires=1)\n    qml.RY(p[15], wires=3)\n    qml.RY(p[16], wires=5)\n    qml.RY(p[17], wires=7)\n\n    # Entagling two-qubit gates\n    # circling the edge of the board\n    qml.CRY(p[18], wires=[0, 1])\n    qml.CRY(p[19], wires=[2, 1])\n    qml.CRY(p[20], wires=[2, 5])\n    qml.CRY(p[21], wires=[8, 5])\n    qml.CRY(p[22], wires=[8, 7])\n    qml.CRY(p[23], wires=[6, 7])\n    qml.CRY(p[24], wires=[6, 3])\n    qml.CRY(p[25], wires=[0, 3])\n\n    # To the corners from the centre\n    qml.CRY(p[26], wires=[4, 0])\n    qml.CRY(p[27], wires=[4, 2])\n    qml.CRY(p[28], wires=[4, 6])\n    qml.CRY(p[29], wires=[4, 8])\n\n    # To the centre from the edges\n    qml.CRY(p[30], wires=[1, 4])\n    qml.CRY(p[31], wires=[3, 4])\n    qml.CRY(p[32], wires=[5, 4])\n    qml.CRY(p[33], wires=[7, 4])\n\n    return [qml.expval(ob_center), qml.expval(ob_corner), qml.expval(ob_edge)]\n\n\nfig, ax = qml.draw_mpl(circuit_no_sym)([0] * 9, [0] * 34)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note again how, though these circuits have a similar form to before,\nthey are parameterised differently. We need to feed the vector\n$\\boldsymbol{y}$ made up of the expectation value of these three\noperators into the loss function and use this to update our parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import math\n\ndef encode_game(game):\n    board, res = game\n    x = board * (2 * math.pi) / 3\n    if res == 1:\n        y = [-1, -1, 1]\n    elif res == -1:\n        y = [1, -1, -1]\n    else:\n        y = [-1, 1, -1]\n    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recall that the loss function we\\'re interested in is\n$\\mathcal{L}(\\mathcal{D})=\\frac{1}{|\\mathcal{D}|} \\sum_{(\\boldsymbol{g}, \\boldsymbol{y}) \\in \\mathcal{D}}\\|\\hat{\\boldsymbol{y}}(\\boldsymbol{g})-\\boldsymbol{y}\\|_{2}^{2}$.\nWe need to define this and then we can begin our optimisation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# calculate the mean square error for this classification problem\ndef cost_function(params, input, target):\n    output = torch.stack([torch.hstack(circuit(x, params)) for x in input])\n    vec = output - target\n    sum_sqr = torch.sum(vec * vec, dim=1)\n    return torch.mean(sum_sqr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s now train our symmetry-preserving circuit on the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch import optim\nimport numpy as np\n\nparams = 0.01 * torch.randn(9)\nparams.requires_grad = True\nopt = optim.Adam([params], lr=1e-2)\n\n\nmax_epoch = 15\nmax_step = 30\nbatch_size = 10\n\nencoded_dataset = list(zip(*[encode_game(game) for game in dataset]))\nencoded_dataset_val = list(zip(*[encode_game(game) for game in dataset_val]))\n\n\ndef accuracy(p, x_val, y_val):\n    with torch.no_grad():\n        y_val = torch.tensor(y_val)\n        y_out = torch.stack([torch.hstack(circuit(x, p)) for x in x_val])\n        acc = torch.sum(torch.argmax(y_out, axis=1) == torch.argmax(y_val, axis=1))\n        return acc / len(x_val)\n\n\nprint(f\"accuracy without training = {accuracy(params, *encoded_dataset_val)}\")\n\nx_dataset = torch.stack(encoded_dataset[0])\ny_dataset = torch.tensor(encoded_dataset[1], requires_grad=False)\n\nsaved_costs_sym = []\nsaved_accs_sym = []\nfor epoch in range(max_epoch):\n    rand_idx = torch.randperm(len(x_dataset))\n    # Shuffled dataset\n    x_dataset = x_dataset[rand_idx]\n    y_dataset = y_dataset[rand_idx]\n\n    costs = []\n\n    for step in range(max_step):\n        x_batch = x_dataset[step * batch_size : (step + 1) * batch_size]\n        y_batch = y_dataset[step * batch_size : (step + 1) * batch_size]\n\n        def opt_func():\n            opt.zero_grad()\n            loss = cost_function(params, x_batch, y_batch)\n            costs.append(loss.item())\n            loss.backward()\n            return loss\n\n        opt.step(opt_func)\n\n    cost = np.mean(costs)\n    saved_costs_sym.append(cost)\n\n    if (epoch + 1) % 1 == 0:\n        # Compute validation accuracy\n        acc_val = accuracy(params, *encoded_dataset_val)\n        saved_accs_sym.append(acc_val)\n\n        res = [epoch + 1, cost, acc_val]\n        print(\"Epoch: {:2d} | Loss: {:3f} | Validation accuracy: {:3f}\".format(*res))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we train the non-symmetry preserving circuit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params = 0.01 * torch.randn(34)\nparams.requires_grad = True\nopt = optim.Adam([params], lr=1e-2)\n\n# calculate mean square error for this classification problem\n\n\ndef cost_function_no_sym(params, input, target):\n    output = torch.stack([torch.hstack(circuit_no_sym(x, params)) for x in input])\n    vec = output - target\n    sum_sqr = torch.sum(vec * vec, dim=1)\n    return torch.mean(sum_sqr)\n\n\nmax_epoch = 15\nmax_step = 30\nbatch_size = 15\n\nencoded_dataset = list(zip(*[encode_game(game) for game in dataset]))\nencoded_dataset_val = list(zip(*[encode_game(game) for game in dataset_val]))\n\n\ndef accuracy_no_sym(p, x_val, y_val):\n    with torch.no_grad():\n        y_val = torch.tensor(y_val)\n        y_out = torch.stack([torch.hstack(circuit_no_sym(x, p)) for x in x_val])\n        acc = torch.sum(torch.argmax(y_out, axis=1) == torch.argmax(y_val, axis=1))\n        return acc / len(x_val)\n\n\nprint(f\"accuracy without training = {accuracy_no_sym(params, *encoded_dataset_val)}\")\n\n\nx_dataset = torch.stack(encoded_dataset[0])\ny_dataset = torch.tensor(encoded_dataset[1], requires_grad=False)\n\nsaved_costs = []\nsaved_accs = []\nfor epoch in range(max_epoch):\n    rand_idx = torch.randperm(len(x_dataset))\n    # Shuffled dataset\n    x_dataset = x_dataset[rand_idx]\n    y_dataset = y_dataset[rand_idx]\n\n    costs = []\n\n    for step in range(max_step):\n        x_batch = x_dataset[step * batch_size : (step + 1) * batch_size]\n        y_batch = y_dataset[step * batch_size : (step + 1) * batch_size]\n\n        def opt_func():\n            opt.zero_grad()\n            loss = cost_function_no_sym(params, x_batch, y_batch)\n            costs.append(loss.item())\n            loss.backward()\n            return loss\n\n        opt.step(opt_func)\n\n    cost = np.mean(costs)\n    saved_costs.append(costs)\n\n    if (epoch + 1) % 1 == 0:\n        # Compute validation accuracy\n        acc_val = accuracy_no_sym(params, *encoded_dataset_val)\n        saved_accs.append(acc_val)\n\n        res = [epoch + 1, cost, acc_val]\n        print(\"Epoch: {:2d} | Loss: {:3f} | Validation accuracy: {:3f}\".format(*res))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally let\\'s plot the results and see how the two training regimes\ndiffer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n\nplt.title(\"Validation accuracies\")\nplt.plot(saved_accs_sym, \"b\", label=\"Symmetric\")\nplt.plot(saved_accs, \"g\", label=\"Standard\")\n\nplt.ylabel(\"Validation accuracy (%)\")\nplt.xlabel(\"Optimization steps\")\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What we can see then is that by paying attention to the symmetries\nintrinsic to the learning problem and reflecting this in an equivariant\ngate set we have managed to improve our learning accuracies, while also\nusing fewer parameters. While the symmetry-aware circuit clearly\noutperforms the naive one, it is notable however that the learning\naccuracies in both cases are hardly ideal given this is a solved game.\nSo paying attention to symmetries definitely helps, but it also isn\\'t a\nmagic bullet!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The use of symmetries in both quantum and classical machine learning is\na developing field, so we can expect new results to emerge over the\ncoming years. If you want to get involved, the references given below\nare a great place to start.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n==========\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Acknowledgments\n===============\n\nThe author would also like to acknowledge the helpful input of C.-Y.\nPark.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "About the author\n================\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}