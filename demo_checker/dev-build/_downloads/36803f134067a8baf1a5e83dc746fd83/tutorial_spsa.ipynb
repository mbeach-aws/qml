{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimization using SPSA {#spsa}\n=======================\n\n::: {.meta}\n:property=\\\"og:description\\\": Use the simultaneous perturbation\nstochastic approximation algorithm to optimize variational circuits in\nPennyLane. :property=\\\"og:image\\\":\n<https://pennylane.ai/qml/_images/spsa_mntn.png>\n:::\n\n::: {.related}\ntutorial\\_vqe A brief overview of VQE tutorial\\_vqe\\_qng Accelerating\nVQEs with quantum natural gradient qnspsa Quantum natural SPSA optimizer\n:::\n\n*Authors: Antal Szava & David Wierichs --- Posted: 19 March 2021. Last\nupdated: 23 February 2023.*\n\nIn this tutorial, we investigate using a stochastic optimizer called the\nSimultaneous Perturbation Stochastic Approximation (SPSA) algorithm to\noptimize quantum circuits. This optimizer is built into PennyLane as\n`~pennylane.SPSAOptimizer`{.interpreted-text role=\"class\"}. SPSA is a\ntechnique that involves approximating the gradient of a quantum circuit\nwithout having to compute it exactly.\n\nThis demonstration shows how the SPSA optimizer performs on the\nfollowing tasks, compared to a gradient descent optimization:\n\n1.  A simple task on a sampling device,\n2.  The variational quantum eigensolver on a simulated hardware device.\n\nThroughout the demo, we show results obtained with SPSA and with\ngradient descent and also compare the number of executed circuits\nrequired to complete each optimization.\n\nBackground\n----------\n\nIn PennyLane, quantum gradients on hardware are commonly computed using\n[parameter-shift\nrules](https://pennylane.ai/qml/glossary/parameter_shift.html).\nComputing quantum gradients involves evaluating the partial derivative\nof the quantum function with respect to every free parameter. These\npartial derivatives are then used to apply the chain rule to compute the\ngradient of the quantum circuit. For qubit operations that are generated\nby one of the Pauli matrices, each partial derivative computation will\ninvolve two quantum circuit evaluations with a positive and a negative\nshift in the parameter values.\n\nAs there are two circuit evaluations for each free parameter, the number\nof overall quantum circuit executions for computing a quantum gradient\ncan be expected to scale as $O(p)$ with the number of free parameters\n$p$. This scaling can be very costly for optimization tasks with many\nfree parameters. For the overall optimization this scaling means we need\n$O(pn)$ quantum circuit evaluations, where $n$ is the number of\noptimization steps taken.\n\nFortunately, there are certain optimization techniques that offer an\nalternative to computing the gradients of quantum circuits. One such\ntechnique is called the Simultaneous Perturbation Stochastic\nApproximation (SPSA) algorithm. SPSA is an optimization method that\ninvolves *approximating* the gradient of the cost function at each\niteration step. This technique requires only two quantum circuit\nexecutions per iteration step, regardless of the number of free\nparameters. Therefore the overall number of circuit executions would be\n$O(n')$ where $n'$ is the number of optimization steps taken when using\nSPSA. This technique is also considered robust against noise, making it\na great optimization method in the NISQ era.\n\nIn this demo, you\\'ll learn how the SPSA algorithm works, and how to\napply it in PennyLane to compute gradients of quantum circuits. You\\'ll\nalso see it in action using noisy quantum data!\n\nSimultaneous perturbation stochastic approximation (SPSA)\n---------------------------------------------------------\n\nSPSA is a general method for minimizing differentiable multivariate\nfunctions. It is particularly useful for functions for which evaluating\nthe gradient is not possible, or too resource intensive. SPSA provides a\nstochastic method for approximating the gradient of the cost function.\nTo accomplish this, the cost function is evaluated twice using perturbed\nparameter vectors: every component of the original parameter vector is\nsimultaneously shifted with a randomly generated value. This is in\ncontrast to finite-differences methods where for each evaluation only\none component of the parameter vector is shifted at a time.\n\nSimilar to gradient-based approaches such as gradient descent, SPSA is\nan iterative optimization algorithm. Let\\'s consider a differentiable\ncost function $L(\\theta)$ where $\\theta$ is a $p$-dimensional vector and\nwhere the optimization problem can be translated into finding a optimal\nparameter setting $\\theta^*$ at which\n$\\frac{\\partial L}{\\partial \\theta} = 0$. It is assumed that\nmeasurements of $L(\\theta)$ are available at various values of\n$\\theta$\\-\\--this is exactly the problem that we\\'d consider when\noptimizing quantum functions!\n\nSPSA starts with an initial parameter vector $\\hat{\\theta}_{0}$. Its\nupdate rule is very similar to the one of standard gradient descent:\n\n$$\\hat{\\theta}_{k+1} = \\hat{\\theta}_{k} - a_{k}\\hat{g}_{k}(\\hat{\\theta}_{k}),$$\n\nwhere $\\hat{g}_{k}$ is the stochastic estimate of the gradient\n$g(\\theta) = \\frac{ \\partial L}{\\partial \\theta}$ at the iterate\n$\\hat{\\theta}_{k}$ based on prior measurements of the cost function, and\n$a_{k}$ is a positive number.\n\nOne of the advantages of SPSA is that it is robust to noise that may\noccur when measuring the function $L$. Therefore, let\\'s consider the\nfunction $y(\\theta)=L(\\theta) + \\varepsilon$, where $\\varepsilon$ is\nsome perturbation of the output. In SPSA, the estimated gradient at each\niteration step is expressed as\n\n$$\\hat{g}_{ki} (\\hat{\\theta}_{k}) = \\frac{y(\\hat{\\theta}_{k} +c_{k}\\Delta_{k})\n- y(\\hat{\\theta}_{k} -c_{k}\\Delta_{k})}{2c_{k}\\Delta_{ki}},$$\n\nwhere $c_{k}$ is a positive number and $\\Delta_{k} = (\\Delta_{k_1},\n\\Delta_{k_2}, ..., \\Delta_{k_p})^{T}$ is a perturbation vector. The\nstochasticity of the technique comes from the fact that for each\niteration step $k$ the components of the $\\Delta_{k}$ perturbation\nvector are randomly generated using a zero-mean distribution. In most\ncases, the Rademacher distribution is used, meaning each parameter is\nsimultaneously perturbed by either $\\pm c_k$.\n\nIt is this perturbation that makes SPSA robust to noise --- since every\nparameter is already being shifted, additional shifts due to noise are\nless likely to hinder the optimization process. In a sense, noise gets\n\\\"absorbed\\\" into the already-stochastic process. This is highlighted in\nthe figure below, which portrays an example of the type of path SPSA\ntakes through the space of the function, compared to a standard\ngradient-based optimizer.\n\n![..](../demonstrations/spsa/spsa_mntn.png){.align-center width=\"60.0%\"}\n\nA schematic of the search paths used by gradient descent with\nparameter-shift and SPSA.\n\nNow that we have explored how SPSA works, let\\'s see how it performs in\npractice!\n\nOptimization on a sampling device\n---------------------------------\n\nFirst, let\\'s consider a simple quantum circuit on a sampling device.\nFor this, we\\'ll be using a device from the [PennyLane-Qiskit\nplugin](https://pennylaneqiskit.readthedocs.io/en/latest/) that samples\nquantum circuits to get measurement outcomes and later post-processes\nthese outcomes to compute statistics like expectation values.\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nJust as with other PennyLane device, the number of samples taken for a\ncircuit execution can be specified using the `shots` keyword argument of\nthe device.\n:::\n\nOnce we have a device selected, we just need a couple of other\ningredients for the pieces of an example optimization to come together:\n\n-   a circuit ansatz:\n    `~pennylane.templates.layers.StronglyEntanglingLayers`{.interpreted-text\n    role=\"func\"},\n-   initial parameters: the correct shape can be computed by the `shape`\n    method of the ansatz. We also use a seed so that we can simulate the\n    same optimization every time (except for the device noise and shot\n    noise).\n-   an observable: $\\bigotimes_{i=0}^{N-1}\\sigma_z^i$, where $N$ stands\n    for the number of qubits.\n-   the number of layers in the ansatz and the number of wires. We\n    choose five layers and four wires.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\nfrom pennylane import numpy as np\n\nnum_wires = 4\nnum_layers = 5\n\ndevice = qml.device(\"qiskit.aer\", wires=num_wires, shots=1000)\n\nansatz = qml.StronglyEntanglingLayers\n\nall_pauliz_tensor_prod = qml.prod(*[qml.PauliZ(i) for i in range(num_wires)])\n\n\ndef circuit(param):\n    ansatz(param, wires=list(range(num_wires)))\n    return qml.expval(all_pauliz_tensor_prod)\n\n\ncost_function = qml.QNode(circuit, device)\n\nnp.random.seed(50)\n\nparam_shape = ansatz.shape(num_layers, num_wires)\ninit_param = np.random.normal(scale=0.1, size=param_shape, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will execute a few optimizations in this demo, so let\\'s prepare a\nconvenience function that runs an optimizer instance and records the\ncost values along the way. Together with the number of executed\ncircuits, this will be an interesting quantity to evaluate the\noptimization cost on hardware!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def run_optimizer(opt, cost_function, init_param, num_steps, interval, execs_per_step):\n    # Copy the initial parameters to make sure they are never overwritten\n    param = init_param.copy()\n\n    # Obtain the device used in the cost function\n    dev = cost_function.device\n\n    # Initialize the memory for cost values during the optimization\n    cost_history = []\n    # Monitor the initial cost value\n    cost_history.append(cost_function(param))\n    exec_history = [0]\n\n    print(f\"\\nRunning the {opt.__class__.__name__} optimizer for {num_steps} iterations.\")\n    for step in range(num_steps):\n        # Print out the status of the optimization\n        if step % interval == 0:\n            print(\n                f\"Step {step:3d}: Circuit executions: {exec_history[step]:4d}, \"\n                f\"Cost = {cost_history[step]}\"\n            )\n\n        # Perform an update step\n        param = opt.step(cost_function, param)\n\n        # Monitor the cost value\n        cost_history.append(cost_function(param))\n        exec_history.append((step + 1) * execs_per_step)\n\n    print(\n        f\"Step {num_steps:3d}: Circuit executions: {exec_history[-1]:4d}, \"\n        f\"Cost = {cost_history[-1]}\"\n    )\n    return cost_history, exec_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we have defined each piece of the optimization, there\\'s only one\nremaining component required: the *SPSA optimizer*. We\\'ll use the\n`~pennylane.SPSAOptimizer`{.interpreted-text role=\"class\"} built into\nPennyLane, for 200 iterations in total.\n\nChoosing the hyperparameters\n============================\n\nThe `SPSAOptimizer` allows us to choose the initial value of two\nhyperparameters for SPSA: the $c$ and $a$ coefficients. Recall from\nabove that the $c$ values control the scale of the random shifts when\nevaluating the cost function, while the $a$ coefficient is analogous to\na learning rate and affects the rate at which the parameters change at\neach update step.\n\nWith stochastic approximation, specifying such hyperparameters\nsignificantly influences the convergence of the optimization for a given\nproblem. Although there is no universal recipe for selecting these\nvalues (as they depend strongly on the specific problem), includes\nguidelines for the selection. In our case, the initial values for $c$\nand $a$ were selected as a result of a grid search to ensure a fast\nconvergence. We further note that apart from $c$ and $a$, there are\nfurther coefficients that are initialized in the `SPSAOptimizer` using\nthe previously mentioned guidelines. For more details, also consider the\n[PennyLane documentation of the\noptimizer](https://docs.pennylane.ai/en/stable/code/api/pennylane.SPSAOptimizer.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_steps_spsa = 200\nopt = qml.SPSAOptimizer(maxiter=num_steps_spsa, c=0.15, a=0.2)\n# We spend 2 circuit evaluations per step:\nexecs_per_step = 2\ncost_history_spsa, exec_history_spsa = run_optimizer(\n    opt, cost_function, init_param, num_steps_spsa, 20, execs_per_step\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let\\'s perform the same optimization using gradient descent. We set\nthe step size according to a favourable value found after grid search\nfor fast convergence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_steps_grad = 15\nopt = qml.GradientDescentOptimizer(stepsize=0.3)\n# We spend 2 circuit evaluations per parameter per step:\nexecs_per_step = 2 * np.prod(param_shape)\ncost_history_grad, exec_history_grad = run_optimizer(\n    opt, cost_function, init_param, num_steps_grad, 3, execs_per_step\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SPSA and gradient descent comparison\n====================================\n\nAt this point, nothing else remains but to check which of these\napproaches did better!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\n\nplt.plot(exec_history_grad, cost_history_grad, label=\"Gradient descent\")\nplt.plot(exec_history_spsa, cost_history_spsa, label=\"SPSA\")\n\nplt.xlabel(\"Circuit executions\", fontsize=14)\nplt.ylabel(\"Cost function value\", fontsize=14)\nplt.grid()\n\nplt.title(\"Gradient descent vs. SPSA for simple optimization\", fontsize=16)\nplt.legend(fontsize=14)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems that SPSA performs great and it does so with significant\nsavings when compared to gradient descent!\n\nLet\\'s take a deeper dive to see how much better it actually is by\ncomputing the ratio of required circuit executions to reach an absolute\naccuracy of 0.01.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grad_execs_to_prec = exec_history_grad[np.where(np.array(cost_history_grad) < -0.99)[0][0]]\nspsa_execs_to_prec = exec_history_spsa[np.where(np.array(cost_history_spsa) < -0.99)[0][0]]\nprint(f\"Circuit execution ratio: {np.round(grad_execs_to_prec/spsa_execs_to_prec, 3)}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This means that SPSA found the minimum up to an absolute accuracy of\n0.01 while using multiple times fewer circuit executions than gradient\ndescent! That\\'s an important saving, especially when running the\nalgorithm on actual quantum hardware.\n\nSPSA and the variational quantum eigensolver\n============================================\n\nNow that we\\'ve explored the theoretical underpinnings of SPSA and its\nuse for a toy problem optimization, let\\'s use it to optimize a real\nchemical system, namely that of the hydrogen molecule $H_2$. This\nmolecule was studied previously in the `introductory variational quantum\neigensolver (VQE) demo </demos/tutorial_vqe>`{.interpreted-text\nrole=\"doc\"}, and so we will reuse some of that machinery below to set up\nthe problem.\n\nThe $H_2$ Hamiltonian uses 4 qubits, contains 15 terms, and has a ground\nstate energy of $-1.136189454088$ Hartree.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pennylane import qchem\n\nsymbols = [\"H\", \"H\"]\ncoordinates = np.array([0.0, 0.0, -0.6614, 0.0, 0.0, 0.6614])\nh2_ham, num_qubits = qchem.molecular_hamiltonian(symbols, coordinates)\nh2_ham = qml.Hamiltonian(qml.math.real(h2_ham.coeffs), h2_ham.ops)\n\ntrue_energy = -1.136189454088\n\n\n# Variational ansatz for H_2 - see Intro VQE demo for more details\ndef ansatz(param, wires):\n    qml.BasisState(np.array([1, 1, 0, 0]), wires=wires)\n    for i in wires:\n        qml.Rot(*param[0, i], wires=i)\n    qml.CNOT(wires=[2, 3])\n    qml.CNOT(wires=[2, 0])\n    qml.CNOT(wires=[3, 1])\n    for i in wires:\n        qml.Rot(*param[1, i], wires=i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since SPSA is robust to noise, let\\'s see how it fares compared to\ngradient descent when run on noisy hardware. For this, we will set up\nand use a simulated version of IBM Q\\'s hardware.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Note: you will need to be authenticated to IBMQ to run the following (commented) code.\n# Do not run the simulation on this device, as it will send it to real hardware\n# For access to IBMQ, the following statements will be useful:\n# from qiskit import IBMQ\n# IBMQ.load_account() # Load account from disk\n# List the providers to pick an available backend:\n# IBMQ.providers()    # List all available providers\n# dev = qml.device(\"qiskit.ibmq\", wires=num_qubits, backend=\"ibmq_lima\")\n\nfrom qiskit.providers.aer import noise\nfrom qiskit.providers.fake_provider import FakeLima\n\n# Load a fake backed to create a noise model, and create a device using that model\nnoise_model = noise.NoiseModel.from_backend(FakeLima())\nnoisy_device = qml.device(\n    \"qiskit.aer\", wires=num_qubits, shots=1000, noise_model=noise_model\n)\n\n\ndef circuit(param):\n    ansatz(param, range(num_qubits))\n    return qml.expval(h2_ham)\n\n\ncost_function = qml.QNode(circuit, noisy_device)\n\n# This random seed was used in the original VQE demo and is known to allow the\n# gradient descent algorithm to converge to the global minimum.\nnp.random.seed(0)\nparam_shape = (2, num_qubits, 3)\ninit_param = np.random.normal(0, np.pi, param_shape, requires_grad=True)\n\n# Initialize the optimizer - optimal step size was found through a grid search\nopt = qml.GradientDescentOptimizer(stepsize=2.2)\n\n# We spend 2 * 15 circuit evaluations per parameter per step, as there are\n# 15 Hamiltonian terms\nexecs_per_step = 2 * 15 * np.prod(param_shape)\n# Run the optimization\ncost_history_grad, exec_history_grad = run_optimizer(\n    opt, cost_function, init_param, num_steps_grad, 3, execs_per_step\n)\n\nfinal_energy = cost_history_grad[-1]\nprint(f\"\\nFinal estimated value of the ground state energy = {final_energy:.8f} Ha\")\nprint(\n    f\"Distance to the true ground state energy: {np.abs(final_energy - true_energy):.8f} Ha\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What does the optimization with gradient descent look like? Let\\'s plot\nthe energy during optimization and compare it to the exact ground state\nenergy of the molecule:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n\nplt.plot(exec_history_grad, cost_history_grad, label=\"Gradient descent\")\n\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.xlabel(\"Circuit executions\", fontsize=14)\nplt.ylabel(\"Energy (Ha)\", fontsize=14)\nplt.grid()\n\nplt.axhline(y=true_energy, color=\"black\", linestyle=\"--\", label=\"True energy\")\n\nplt.legend(fontsize=14)\n\nplt.title(\"H2 energy from VQE with gradient descent\", fontsize=16)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On noisy hardware, the energy never quite reaches its true value, no\nmatter how many iterations are used. This is due to the noise as well as\nthe stochastic nature of quantum measurements and the way they are\nrealized on hardware. The simulator of the noisy quantum device allows\nus to observe these features.\n\nVQE with SPSA\n=============\n\nNow let\\'s perform the same experiment using SPSA for the VQE\noptimization. SPSA should use only 2 circuit executions per term in the\nexpectation value. Since there are 15 terms and we choose 160 iterations\nwith two evaluations for each gradient estimate, we expect 4800 total\ndevice executions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_steps_spsa = 160\nopt = qml.SPSAOptimizer(maxiter=num_steps_spsa, c=0.3, a=1.5)\n\n# We spend 2 * 15 circuit evaluations per step, as there are 15 Hamiltonian terms\nexecs_per_step = 2 * 15\n# Run the optimization\ncost_history_spsa, exec_history_spsa = run_optimizer(\n    opt, cost_function, init_param, num_steps_spsa, 20, execs_per_step\n)\nfinal_energy = cost_history_spsa[-1]\n\nprint(f\"\\nFinal estimated value of the ground state energy = {final_energy:.8f} Ha\")\nprint(\n    f\"Distance to the true ground state energy: {np.abs(final_energy - true_energy):.8f} Ha\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The SPSA optimization seems to have found a similar energy value. We\nagain take a look at how the optimization curves compare, in particular\nwith respect to the circuit executions spent on the task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n\nplt.plot(exec_history_grad, cost_history_grad, label=\"Gradient descent\")\nplt.plot(exec_history_spsa, cost_history_spsa, label=\"SPSA\")\nplt.axhline(y=true_energy, color=\"black\", linestyle=\"--\", label=\"True energy\")\n\nplt.title(\"$H_2$ energy from VQE using gradient descent vs. SPSA\", fontsize=16)\nplt.xlabel(\"Circuit executions\", fontsize=14)\nplt.ylabel(\"Energy (Ha)\", fontsize=14)\nplt.grid()\n\nplt.legend(fontsize=14)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We observe here that the SPSA optimizer again converges in fewer device\nexecutions than the gradient descent optimizer. \ud83c\udf89\n\nDue to the (simulated) hardware noise, however, the obtained energies\nare higher than the true ground state energy. In addition, the output\nstill bounces around, which is due to shot noise and the inherently\nstochastic nature of SPSA.\n\nConclusion\n==========\n\nSPSA is a useful optimization technique that may be particularly\nbeneficial on near-term quantum hardware. It uses significantly fewer\ncircuit executions to achieve comparable results as gradient-based\nmethods, giving it the potential to save time and resources. It can be a\ngood alternative to gradient-based methods when the optimization problem\ninvolves executing quantum circuits with many free parameters.\n\nThere are also extensions to SPSA that could be interesting to explore\nin this context. One, in particular, uses an adaptive technique to\napproximate the *Hessian* matrix during optimization to effectively\nincrease the convergence rate of SPSA[^1].\n\nIn addition, there is a proposal to use an SPSA variant of the quantum\nnatural gradient[^2]. This is implemented in PennyLane as well and we\ndiscuss it in the `demo on QNSPSA </demos/qnspsa>`{.interpreted-text\nrole=\"doc\"}.\n\nReferences\n==========\n\n[^1]: J. C. Spall, \\\"Adaptive stochastic approximation by the\n    simultaneous perturbation method,\\\" in IEEE Transactions on\n    Automatic Control, vol. 45, no. 10, pp. 1839-1853, Oct 2020, doi:\n    10.1109/TAC.2000.880982.\n\n[^2]: J. Gacon, C. Zoufal, G. Carleo, and S. Woerner \\\"Simultaneous\n    Perturbation Stochastic Approximation of the Quantum Fisher\n    Information\\\", [Quantum, 5,\n    567](https://quantum-journal.org/papers/q-2021-10-20-567/), Oct 2021\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "About the author\n================\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}