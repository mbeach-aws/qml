{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function Fitting using Quantum Signal Processing\n================================================\n\n::: {.meta}\n:property=\\\"og:description\\\": Learn how to create polynomial\napproximations to functions using Quantum Signal Processing (QSP).\n:property=\\\"og:image\\\":\n<https://pennylane.ai/qml/demonstrations/function_fitting_qsp/cover.png>\n:::\n\n*Author: Jay Soni --- Posted: 24 May 2022. Last updated: 17 April 2023.*\n\nIntroduction\n------------\n\nThis demo is inspired by the paper ['A Grand Unification of Quantum\nAlgorithms'](https://arxiv.org/abs/2105.02859). This paper is centered\naround the Quantum Singular Value Transform (QSVT) protocol and how it\nprovides a single framework to generalize some of the most famous\nquantum algorithms like Shor's factoring algorithm, Grover search, and\nmore.\n\nThe QSVT is a method to apply polynomial transformations to the singular\nvalues of *any matrix*. This is powerful because from polynomial\ntransformations we can generate arbitrary function transformations using\nTaylor approximations. The QSVT protocol is an extension of the more\nconstrained Quantum Signal Processing (QSP) protocol which presents a\nmethod for polynomial transformation of matrix entries in a single-qubit\nunitary operator. The QSVT protocol is sophisticated, but the idea at\nits core is quite simple. By studying QSP, we get a relatively simpler\npath to explore this idea at the foundation of QSVT.\n\nIn this demo, we explore the QSP protocol and how it can be used for\ncurve fitting. We show how you can fit polynomials, as illustrated in\nthe animation below.\n\n![](../demonstrations/function_fitting_qsp/trained_poly.gif){.align-center\nwidth=\"50.0%\"}\n\nThis is a powerful tool that will ultimately allow us to approximate any\nfunction on the interval $[-1, 1]$ that satisfies certain constraints.\nBefore we can dive into function fitting, let's develop some intuition.\nConsider the following single-qubit operator parameterized by\n$a \\in [-1, 1]$:\n\n$$\\begin{aligned}\n\\hat{W}(a) = \\begin{bmatrix} a & i\\sqrt{1 - a^{2}} \\\\ i\\sqrt{1 - a^{2}} & a \\end{bmatrix}.\n\\end{aligned}$$\n\n$\\hat{W}(a)$ is called the *signal rotation operator* (SRO). Using this\noperator, we can construct another operator which we call *signal\nprocessing operator* (SPO),\n\n$$\\hat{U}_{sp} = \\hat{R}_{z}(\\phi_{0}) \\prod_{k=1}^{d} \\hat{W}(a) \\hat{R}_{z}(\\phi_{k}).$$\n\nThe SPO is parameterized by a vector $\\vec{\\phi} \\in \\mathbb{R}^{d+1}$,\nwhere $d$ is a free parameter which represents the number of repeated\napplications of $\\hat{W}(a)$.\n\nThe SPO $\\hat{U}_{sp}$ alternates between applying the SRO $\\hat{W}(a)$\nand parameterized rotations around the z-axis. Let's see what happens\nwhen we try to compute the expectation value\n$\\langle 0|\\hat{U}_{sp}|0\\rangle$ for the particular case where $d = 2$\nand $\\vec{\\phi} = (0, 0, 0)$ :\n\n$$\\begin{aligned}\n\\begin{align*}\n\\langle 0 |\\hat{U}_{sp}|0\\rangle &= \\langle 0 | \\ \\hat{R}_{z}(0) \\prod_{k=1}^{2} \\hat{W}(a) \\hat{R}_{z}(0) \\ |0\\rangle \\\\\n\\langle 0 |\\hat{U}_{sp}|0\\rangle &= \\langle 0 | \\hat{W}(a)^{2} |0\\rangle \\\\\n\\end{align*}\n\\end{aligned}$$\n\n$$\\begin{aligned}\n\\langle 0 |\\hat{U}_{sp}|0\\rangle = \\langle 0 | \\begin{bmatrix} a & i\\sqrt{1 - a^{2}} \\\\ i\\sqrt{1 - a^{2}} & a \\end{bmatrix} \\ \\circ \\ \\begin{bmatrix} a & i\\sqrt{1 - a^{2}} \\\\ i\\sqrt{1 - a^{2}} & a \\end{bmatrix} |0\\rangle\n\\end{aligned}$$\n\n$$\\begin{aligned}\n\\langle 0|\\hat{U}_{sp}|0\\rangle = \\langle 0| \\begin{bmatrix} 2a^{2} - 1 & 2ai\\sqrt{1 - a^{2}} \\\\ 2ai\\sqrt{1 - a^{2}} & 2a^{2} - 1 \\end{bmatrix} |0\\rangle\n\\end{aligned}$$\n\n$$\\langle 0|\\hat{U}_{sp}|0\\rangle = 2a^{2} - 1$$\n\nNotice that this quantity is a polynomial in $a$. Equivalently, suppose\nwe wanted to create a map $S: a \\to 2a^2 - 1$. This expectation value\nwould give us the means to perform such a mapping. This may seem oddly\nspecific at first, but it turns out that this process can be generalized\nfor generating a mapping $S: a \\to \\text{poly}(a)$. The following\ntheorem shows us how:\n\n### Theorem: Quantum Signal Processing\n\nGiven a vector $\\vec{\\phi} \\in \\mathbb{R}^{d+1}$, there exist complex\npolynomials $P(a)$ and $Q(a)$ such that the SPO, $\\hat{U}_{sp}$, can be\nexpressed in matrix form as:\n\n$$\\hat{U}_{sp} = \\hat{R}_{z}(\\phi_{0}) \\prod_{k=1}^{d} \\hat{W}(a) \\hat{R}_{z}(\\phi_{k}),$$\n\n$$\\begin{aligned}\n\\hat{U}_{sp} = \\begin{bmatrix} P(a) & iQ(a)\\sqrt{1 - a^{2}} \\\\ iQ^{*}(a)\\sqrt{1 - a^{2}} & P^{*}(a) \\end{bmatrix},\n\\end{aligned}$$\n\nwhere $a \\in [-1, 1]$ and the polynomials $P(a)$, $Q(a)$ satisfy the\nfollowing constraints:\n\n-   $deg(P) \\leq d \\ $ and $deg(Q) \\leq d - 1$,\n-   $P$ has parity $d$ mod 2 and $Q$ has parity, $d - 1$ mod 2\n-   $|P|^{2} + (1 - a^{2})|Q|^{2} = 1$.\n\nThe third condition is actually quite restrictive because if we\nsubstitute $a = \\pm 1$, we get the result $|P^{2}(\\pm 1)| = 1$. Thus it\nrestricts the polynomial to be pinned to $\\pm 1$ at the end points of\nthe domain, $a = \\pm 1$. This condition can be relaxed to\n$|P^{2}(a)| \\leq 1$ by expressing the signal processing operator in the\nHadamard basis, i.e., $\\langle + |\\hat{U}_{sp}(\\vec{\\phi};a)|+\\rangle$).\nThis is equivalent to redefining $P(a)$ such that:\n\n$$P^{'}(a) = \\text{Re}(P(a)) + i\\text{Re}(Q(a))\\sqrt{1 - a^{2}}$$\n\n*This is the convention we follow in this demo.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s Plot some Polynomials\n============================\n\nNow we put this theorem to the test! In this section we construct the\nSRO $\\hat{W}(a)$, and then use PennyLane to define the SPO. To test the\ntheorem we will randomly generate parameters $\\vec{\\phi}$ and plot the\nexpectation value $\\langle + |\\hat{U}_{sp}(\\vec{\\phi};a)|+\\rangle$ for\n$a \\in [-1, 1]$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we introduce a function called `rotation_mat(a)`, which will\nconstruct the SRO matrix. We can also make a helper function\n(`generate_many_sro(a_vals)`) which, given an array of possible values\nfor '$a$', will generate an array of $\\hat{W}(a)$ associated with each\nelement. We use Pytorch to construct this array as it will later be used\nas input when training our function fitting model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\n\ndef rotation_mat(a):\n    \"\"\"Given a fixed value 'a', compute the signal rotation matrix W(a).\n    (requires -1 <= 'a' <= 1)\n    \"\"\"\n    diag = a\n    off_diag = (1 - a**2) ** (1 / 2) * 1j\n    W = [[diag, off_diag], [off_diag, diag]]\n\n    return W\n\n\ndef generate_many_sro(a_vals):\n    \"\"\"Given a tensor of possible 'a' vals, return a tensor of W(a)\"\"\"\n    w_array = []\n    for a in a_vals:\n        w = rotation_mat(a)\n        w_array.append(w)\n\n    return torch.tensor(w_array, dtype=torch.complex64, requires_grad=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now having access to the matrix elements of the SRO, we can leverage\nPennyLane to define a quantum function that will compute the SPO. Recall\nwe are measuring in the Hadamard basis to relax the third condition of\nthe theorem. We accomplish this by sandwiching the SPO between two\nHadamard gates to account for this change of basis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\n\n\ndef QSP_circ(phi, W):\n    \"\"\"This circuit applies the SPO. The components in the matrix\n    representation of the final unitary are polynomials!\n    \"\"\"\n    qml.Hadamard(wires=0)  # set initial state |+>\n    for angle in phi[:-1]:\n        qml.RZ(angle, wires=0)\n        qml.QubitUnitary(W, wires=0)\n\n    qml.RZ(phi[-1], wires=0)  # final rotation\n    qml.Hadamard(wires=0)  # change of basis |+> , |->\n    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we randomly generate the vector $\\vec{\\phi}$ and plot the\nexpectation value $\\langle +|\\hat{U}_{sp}|+\\rangle$ as a function of\n$a$. In this case we choose $d = 5$. We expect to observe the following:\n\n-   Since $d$ is odd, we expect all of the polynomials we plot to have\n    odd symmetry\n-   Since $d = 5$, we expect none of the polynomials will have terms \\~\n    $O(a^6)$ or higher\n-   All of the polynomials are bounded by $\\pm1$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import math\nimport matplotlib.pyplot as plt\n\nd = 5\na_vals = torch.linspace(-1, 1, 50)\nw_mats = generate_many_sro(a_vals)\n\ngen = torch.Generator()\ngen.manual_seed(444422)  # set random seed for reproducibility\n\nfor i in range(5):\n    phi = torch.rand(d + 1, generator=gen) * 2 * torch.tensor([math.pi], requires_grad=False)\n    matrix_func = qml.matrix(QSP_circ)\n    y_vals = [matrix_func(phi, w)[0, 0].real for w in w_mats]\n\n    plt.plot(a_vals, y_vals, label=f\"poly #{i}\")\n\nplt.vlines(0.0, -1.0, 1.0, color=\"black\")\nplt.hlines(0.0, -1.0, 1.0, color=\"black\")\nplt.legend(loc=1)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](../demonstrations/function_fitting_qsp/random_poly.png){.align-center\nwidth=\"50.0%\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exactly as predicted, all of these conditions are met!\n\n-   All curves have odd symmetry\n-   Qualitatively, the plots look similar to polynomials of low degree\n-   Each plot does not exceed $\\pm1$ !\n\nFunction Fitting with Quantum Signal Processing\n===============================================\n\nAnother observation we can make about this theorem is the fact that it\nholds true in both directions: If we have two polynomials $P(a)$ and\n$Q(a)$ that satisfy the conditions of the theorem, then there exists a\n$\\vec{\\phi}$ for which we can construct a signal processing operator\nwhich maps $a \\to P(a)$.\n\nIn this section we try to answer the question:\n\n**Can we learn the parameter values of** $\\vec{\\phi}$ **to transform our\nsignal processing operator polynomial to fit a given function?**\n\nIn order to answer this question, we leverage the power of machine\nlearning. In this demo we assume you are familiar with some concepts\nfrom quantum machine learning, for a refresher checkout this [blog post\non\nQML](https://pennylane.ai/blog/2021/10/how-to-start-learning-quantum-machine-learning/).\nWe begin by building a machine learning model using Pytorch. The\n`__init__()` method handles the random initialization of our parameter\nvector $\\vec{\\phi}$. The `forward()` method takes an array of signal\nrotation matrices $\\hat{W}(a)$ for varying $a$, and produces the\npredicted $y$ values.\n\nNext we leverage the PennyLane function\n[qml.matrix()](https://pennylane.readthedocs.io/en/stable/code/api/pennylane.matrix.html?highlight=qml%20matrix#pennylane.matrix),\nwhich accepts our quantum function (it can also accept quantum tapes and\nQNodes) and returns its unitary matrix representation. We are interested\nin the real value of the top left entry, this corresponds to $P(a)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch_pi = torch.Tensor([math.pi])\n\n\nclass QSP_Func_Fit(torch.nn.Module):\n    def __init__(self, degree, num_vals, random_seed=None):\n        \"\"\"Given the degree and number of samples, this method randomly\n        initializes the parameter vector (randomness can be set by random_seed)\n        \"\"\"\n        super().__init__()\n        if random_seed is None:\n            self.phi = torch_pi * torch.rand(degree + 1, requires_grad=True)\n\n        else:\n            gen = torch.Generator()\n            gen.manual_seed(random_seed)\n            self.phi = torch_pi * torch.rand(degree + 1, requires_grad=True, generator=gen)\n\n        self.phi = torch.nn.Parameter(self.phi)\n        self.num_phi = degree + 1\n        self.num_vals = num_vals\n\n    def forward(self, omega_mats):\n        \"\"\"PennyLane forward implementation\"\"\"\n        y_pred = []\n        generate_qsp_mat = qml.matrix(QSP_circ)\n\n        for w in omega_mats:\n            u_qsp = generate_qsp_mat(self.phi, w)\n            P_a = u_qsp[0, 0]  # Taking the (0,0) entry of the matrix corresponds to <0|U|0>\n            y_pred.append(P_a.real)\n\n        return torch.stack(y_pred, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we create a `Model_Runner` class to handle running the\noptimization, storing the results, and providing plotting functionality:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Model_Runner:\n    def __init__(self, model, degree, num_samples, x_vals, process_x_vals, y_true):\n        \"\"\"Given a model and a series of model specific arguments, store everything in\n        internal attributes.\n        \"\"\"\n        self.model = model\n        self.degree = degree\n        self.num_samples = num_samples\n\n        self.x_vals = x_vals\n        self.inp = process_x_vals(x_vals)\n        self.y_true = y_true\n\n    def execute(\n        self, random_seed=13_02_1967, max_shots=25000, verbose=True\n    ):  # easter egg: oddly specific seed?\n        \"\"\"Run the optimization protocol on the model using Mean Square Error as a loss\n        function and using stochastic gradient descent as the optimizer.\n        \"\"\"\n        model = self.model(degree=self.degree, num_vals=self.num_samples, random_seed=random_seed)\n\n        criterion = torch.nn.MSELoss(reduction=\"sum\")\n        optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n\n        t = 0\n        loss_val = 1.0\n\n        while (t <= max_shots) and (loss_val > 0.5):\n\n            self.y_pred = model(self.inp)\n\n            if t == 1:\n                self.init_y_pred = self.y_pred\n\n            # Compute and print loss\n            loss = criterion(self.y_pred, self.y_true)\n            loss_val = loss.item()\n\n            if (t % 1000 == 0) and verbose:\n                print(f\"---- iter: {t}, loss: {round(loss_val, 4)} -----\")\n\n            # Perform a backward pass and update weights.\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            t += 1\n\n        self.model_params = model.phi\n\n    def plot_result(self, show=True):\n        \"\"\"Plot the results\"\"\"\n        plt.plot(self.x_vals, self.y_true.tolist(), \"--b\", label=\"target func\")\n        plt.plot(self.x_vals, self.y_pred.tolist(), \".g\", label=\"optim params\")\n        plt.plot(self.x_vals, self.init_y_pred.tolist(), \".r\", label=\"init params\")\n        plt.legend(loc=1)\n\n        if show:\n            plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have a model, lets first attempt to fit a polynomial. We\nexpect this to perform well when the target polynomial also obeys the\nsymmetry and degree constraints that our quantum signal processing\npolynomial does. To do this, we defined a function `custom_poly(x)`\nwhich implements the target polynomial. In this case, we (arbitrarily)\nchoose the target polynomial:\n\n$$y = 4x^{5} - 5x^{3} + x$$\n\nLets see how well we can fit this polynomial!\n\n> ::: {.note}\n> ::: {.title}\n> Note\n> :::\n>\n> Depending on the initial parameters, training can take anywhere from\n> 10 - 30 mins\n> :::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nd = 9  # dim(phi) = d + 1,\nnum_samples = 50\n\n\ndef custom_poly(x):\n    \"\"\"A custom polynomial of degree <= d and parity d % 2\"\"\"\n    return torch.tensor(4 * x**5 - 5 * x**3 + x, requires_grad=False, dtype=torch.float)\n\n\na_vals = np.linspace(-1, 1, num_samples)\ny_true = custom_poly(a_vals)\n\nqsp_model_runner = Model_Runner(QSP_Func_Fit, d, num_samples, a_vals, generate_many_sro, y_true)\n\nqsp_model_runner.execute()\nqsp_model_runner.plot_result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.rst-class}\nsphx-glr-script-out\n\nOut:\n\n``` {.none}\n---- iter: 0, loss: 13.5938 -----\n---- iter: 1000, loss: 11.8809 -----\n---- iter: 2000, loss: 10.229 -----\n---- iter: 3000, loss: 8.6693 -----\n---- iter: 4000, loss: 7.2557 -----\n---- iter: 5000, loss: 6.0084 -----\n---- iter: 6000, loss: 4.9197 -----\n---- iter: 7000, loss: 3.9801 -----\n---- iter: 8000, loss: 3.1857 -----\n---- iter: 9000, loss: 2.5312 -----\n---- iter: 10000, loss: 2.0045 -----\n---- iter: 11000, loss: 1.5873 -----\n---- iter: 12000, loss: 1.2594 -----\n---- iter: 13000, loss: 1.0021 -----\n---- iter: 14000, loss: 0.7997 -----\n---- iter: 15000, loss: 0.6397 -----\n---- iter: 16000, loss: 0.5127 -----\n```\n\n![](../demonstrations/function_fitting_qsp/trained_poly.png){.align-center\nwidth=\"50.0%\"}\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We were able to fit that polynomial quite well! Lets try something more\nchallenging: fitting a non-polynomial function. One thing to keep in\nmind is the symmetry and bounds constraints on our polynomials. If our\ntarget function does not satisfy them as well, then we cannot hope to\ngenerate a good polynomial fit, regardless of how long we train for.\n\nA good non-polynomial candidate to fit to, that obeys our constraints,\nis the step function. Let's try it!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "d = 9  # dim(phi) = d + 1,\nnum_samples = 50\n\n\ndef step_func(x):\n    \"\"\"A step function (odd parity) which maps all values <= 0 to -1\n    and all values > 0 to +1.\n    \"\"\"\n    res = [-1.0 if x_i <= 0 else 1.0 for x_i in x]\n    return torch.tensor(res, requires_grad=False, dtype=torch.float)\n\n\na_vals = np.linspace(-1, 1, num_samples)\ny_true = step_func(a_vals)\n\nqsp_model_runner = Model_Runner(QSP_Func_Fit, d, num_samples, a_vals, generate_many_sro, y_true)\n\nqsp_model_runner.execute()\nqsp_model_runner.plot_result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.rst-class}\nsphx-glr-script-out\n\nOut:\n\n``` {.none}\n---- iter: 0, loss: 33.8345 -----\n---- iter: 1000, loss: 19.0937 -----\n---- iter: 2000, loss: 11.6557 -----\n---- iter: 3000, loss: 8.2853 -----\n---- iter: 4000, loss: 6.6824 -----\n---- iter: 5000, loss: 5.8523 -----\n---- iter: 6000, loss: 5.3855 -----\n---- iter: 7000, loss: 5.1036 -----\n---- iter: 8000, loss: 4.9227 -----\n---- iter: 9000, loss: 4.8004 -----\n---- iter: 10000, loss: 4.7138 -----\n---- iter: 11000, loss: 4.6502 -----\n---- iter: 12000, loss: 4.6018 -----\n---- iter: 13000, loss: 4.5638 -----\n---- iter: 14000, loss: 4.5333 -----\n---- iter: 15000, loss: 4.5082 -----\n---- iter: 16000, loss: 4.4872 -----\n---- iter: 17000, loss: 4.4693 -----\n---- iter: 18000, loss: 4.4537 -----\n---- iter: 19000, loss: 4.4401 -----\n---- iter: 20000, loss: 4.4281 -----\n---- iter: 21000, loss: 4.4174 -----\n---- iter: 22000, loss: 4.4078 -----\n---- iter: 23000, loss: 4.3991 -----\n---- iter: 24000, loss: 4.3912 -----\n---- iter: 25000, loss: 4.3839 -----\n```\n\n![](../demonstrations/function_fitting_qsp/trained_step.png){.align-center\nwidth=\"50.0%\"}\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conclusion\n==========\n\nIn this demo, we explored the Quantum Signal Processing theorem, which\nis a method to perform polynomial transformations on the entries of the\nSRO $\\hat{W}(a)$. This polynomial transformation arises from the\nrepeated application of $\\hat{W}(a)$ and the parameterized Z-axis\nrotations $e^{i \\phi \\hat{Z}}$. Note, the SRO is itself a\ntransformation, in this case a rotation around the X-axis by\n$\\theta = -2 \\cos^{-1}(a)$, which rotates our basis. Thus the underlying\nprincipal of quantum signal processing is that we can generate\npolynomial transformations through parameterized rotations along a\nprincipal axis followed by change of basis transformations which\nre-orients this axis.\n\nThis is the same principal at the heart of QSVT. In this case the\nsubspace in which we apply our parameterized rotations is defined by the\nsingular vectors, the change of basis transformation takes us between\nthese subspaces and this allows us to apply polynomial transformations\non the singular values of our matrix of interest.\n\nWe also showed that one could use a simple gradient descent model to\ntrain a parameter vector $\\vec{\\phi}$ to generate reasonably good\npolynomial approximations of arbitrary functions (provided the function\nsatisfied the same constraints). This isn\\'t the only way to compute the\noptimal values. It turns out there exist *efficient* algorithms for\nexplicitly computing the optimal values for $\\vec{\\phi}$ known as\n\\\"Remez-type exchange algorithms\\\" for analytic function fitting. If you\nwant to explore other approaches to function fitting, checkout this\n[demo](https://pennylane.ai/qml/demos/quantum_neural_net.html) where we\nuse a photonic neural network for function fitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](../demonstrations/function_fitting_qsp/trained_step.gif){.align-center\nwidth=\"50.0%\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n==========\n\n\\[1\\]: *John M. Martyn, Zane M. Rossi, Andrew K. Tan, Isaac L. Chuang.\n\"A Grand Unification of Quantum Algorithms\"* [PRX Quantum 2,\n040203](https://arxiv.org/abs/2105.02859)*, 2021.*\n\nAbout the author\n================\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}